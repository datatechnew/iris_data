{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris.sample",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofW_1Zg5IGeT",
        "colab_type": "text"
      },
      "source": [
        "Iris dataset description can be found in\n",
        "http://archive.ics.uci.edu/ml/datasets/iris \n",
        "\n",
        "\n",
        "Iris data can be downloaded from \n",
        "http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkd98e_OtZcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKaK-kzM0iSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "e9ec4a99-328a-43d4-fc66-cddbedf17f0e"
      },
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "new_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'iris_class']\n",
        "dataset = pd.read_csv(url, names = new_names, skiprows =0, delimiter =',')\n",
        "dataset.info()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   iris_class    150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rudr33CG0iV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "526bc3db-49bd-40a7-99bd-c0d0beaab69c"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width   iris_class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE2jotM2JFkm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0d5f5ea1-50d7-429d-ee9d-de8ca6c0dcb7"
      },
      "source": [
        "y = dataset['iris_class']\n",
        "x = dataset.drop(['iris_class'], axis=1)\n",
        "\n",
        "print (\"dataset : \",dataset.shape)\n",
        "print (\"x : \",x.shape)\n",
        "print (\"y : \",y.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset :  (150, 5)\n",
            "x :  (150, 4)\n",
            "y :  (150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8GREreKJFnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "1c779f5c-64f2-4c7c-e45a-f7550f20243b"
      },
      "source": [
        "#one hot encoding\n",
        "y=pd.get_dummies(y)\n",
        "y.sample(7)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "69             0                1               0\n",
              "108            0                0               1\n",
              "91             0                1               0\n",
              "23             1                0               0\n",
              "56             0                1               0\n",
              "122            0                0               1\n",
              "139            0                0               1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1rYqJojJFqz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "550234d5-cb3f-4a5a-f064-d7735f5a55ff"
      },
      "source": [
        "#Selective import Scikit Learn \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate Training and Validation Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) #0.3 data as data test\n",
        "\n",
        "#converting to float 32bit\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "x_test  = np.array(x_test).astype(np.float32)\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "y_test  = np.array(y_test).astype(np.float32)\n",
        "\n",
        "#print data split for validation\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(105, 4) (105, 3)\n",
            "(45, 4) (45, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkAnWwsWJFtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a584430a-02dd-4ab7-805f-221f098e786f"
      },
      "source": [
        "\n",
        "#Importing our model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#model initialization\n",
        "Model = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=1,  random_state=121) #try verbode=0 to train with out logging\n",
        "#train our model\n",
        "h=Model.fit(x_train,y_train)\n",
        "#use our model to predict\n",
        "y_pred=Model.predict(x_test)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 2.42196876\n",
            "Iteration 2, loss = 2.39359469\n",
            "Iteration 3, loss = 2.35627147\n",
            "Iteration 4, loss = 2.31295351\n",
            "Iteration 5, loss = 2.26651726\n",
            "Iteration 6, loss = 2.21926964\n",
            "Iteration 7, loss = 2.17330142\n",
            "Iteration 8, loss = 2.12997064\n",
            "Iteration 9, loss = 2.08947614\n",
            "Iteration 10, loss = 2.05275780\n",
            "Iteration 11, loss = 2.02032962\n",
            "Iteration 12, loss = 1.99204558\n",
            "Iteration 13, loss = 1.96791169\n",
            "Iteration 14, loss = 1.94799308\n",
            "Iteration 15, loss = 1.93153219\n",
            "Iteration 16, loss = 1.91828138\n",
            "Iteration 17, loss = 1.90772587\n",
            "Iteration 18, loss = 1.89989587\n",
            "Iteration 19, loss = 1.89407835\n",
            "Iteration 20, loss = 1.88940421\n",
            "Iteration 21, loss = 1.88541710\n",
            "Iteration 22, loss = 1.88186445\n",
            "Iteration 23, loss = 1.87844026\n",
            "Iteration 24, loss = 1.87505504\n",
            "Iteration 25, loss = 1.87155894\n",
            "Iteration 26, loss = 1.86787789\n",
            "Iteration 27, loss = 1.86401618\n",
            "Iteration 28, loss = 1.86002459\n",
            "Iteration 29, loss = 1.85594100\n",
            "Iteration 30, loss = 1.85176673\n",
            "Iteration 31, loss = 1.84752124\n",
            "Iteration 32, loss = 1.84323228\n",
            "Iteration 33, loss = 1.83901173\n",
            "Iteration 34, loss = 1.83484411\n",
            "Iteration 35, loss = 1.83072138\n",
            "Iteration 36, loss = 1.82665972\n",
            "Iteration 37, loss = 1.82262446\n",
            "Iteration 38, loss = 1.81860512\n",
            "Iteration 39, loss = 1.81461006\n",
            "Iteration 40, loss = 1.81063823\n",
            "Iteration 41, loss = 1.80668754\n",
            "Iteration 42, loss = 1.80275973\n",
            "Iteration 43, loss = 1.79890317\n",
            "Iteration 44, loss = 1.79519416\n",
            "Iteration 45, loss = 1.79153493\n",
            "Iteration 46, loss = 1.78792606\n",
            "Iteration 47, loss = 1.78437591\n",
            "Iteration 48, loss = 1.78086806\n",
            "Iteration 49, loss = 1.77740843\n",
            "Iteration 50, loss = 1.77420643\n",
            "Iteration 51, loss = 1.77129125\n",
            "Iteration 52, loss = 1.76844824\n",
            "Iteration 53, loss = 1.76565279\n",
            "Iteration 54, loss = 1.76291625\n",
            "Iteration 55, loss = 1.76012344\n",
            "Iteration 56, loss = 1.75729851\n",
            "Iteration 57, loss = 1.75445602\n",
            "Iteration 58, loss = 1.75160864\n",
            "Iteration 59, loss = 1.74876728\n",
            "Iteration 60, loss = 1.74594945\n",
            "Iteration 61, loss = 1.74318595\n",
            "Iteration 62, loss = 1.74043794\n",
            "Iteration 63, loss = 1.73769993\n",
            "Iteration 64, loss = 1.73496927\n",
            "Iteration 65, loss = 1.73223350\n",
            "Iteration 66, loss = 1.72951484\n",
            "Iteration 67, loss = 1.72682593\n",
            "Iteration 68, loss = 1.72412861\n",
            "Iteration 69, loss = 1.72141262\n",
            "Iteration 70, loss = 1.71867520\n",
            "Iteration 71, loss = 1.71591371\n",
            "Iteration 72, loss = 1.71314753\n",
            "Iteration 73, loss = 1.71038102\n",
            "Iteration 74, loss = 1.70763036\n",
            "Iteration 75, loss = 1.70486034\n",
            "Iteration 76, loss = 1.70207050\n",
            "Iteration 77, loss = 1.69928130\n",
            "Iteration 78, loss = 1.69647263\n",
            "Iteration 79, loss = 1.69364570\n",
            "Iteration 80, loss = 1.69080032\n",
            "Iteration 81, loss = 1.68793640\n",
            "Iteration 82, loss = 1.68506593\n",
            "Iteration 83, loss = 1.68217683\n",
            "Iteration 84, loss = 1.67927944\n",
            "Iteration 85, loss = 1.67635991\n",
            "Iteration 86, loss = 1.67343620\n",
            "Iteration 87, loss = 1.67051611\n",
            "Iteration 88, loss = 1.66757477\n",
            "Iteration 89, loss = 1.66461216\n",
            "Iteration 90, loss = 1.66162723\n",
            "Iteration 91, loss = 1.65862448\n",
            "Iteration 92, loss = 1.65560465\n",
            "Iteration 93, loss = 1.65256027\n",
            "Iteration 94, loss = 1.64949310\n",
            "Iteration 95, loss = 1.64640310\n",
            "Iteration 96, loss = 1.64328870\n",
            "Iteration 97, loss = 1.64015052\n",
            "Iteration 98, loss = 1.63698859\n",
            "Iteration 99, loss = 1.63380243\n",
            "Iteration 100, loss = 1.63059173\n",
            "Iteration 101, loss = 1.62735916\n",
            "Iteration 102, loss = 1.62410745\n",
            "Iteration 103, loss = 1.62083570\n",
            "Iteration 104, loss = 1.61753164\n",
            "Iteration 105, loss = 1.61419458\n",
            "Iteration 106, loss = 1.61082527\n",
            "Iteration 107, loss = 1.60742692\n",
            "Iteration 108, loss = 1.60401791\n",
            "Iteration 109, loss = 1.60057801\n",
            "Iteration 110, loss = 1.59710416\n",
            "Iteration 111, loss = 1.59360018\n",
            "Iteration 112, loss = 1.59009223\n",
            "Iteration 113, loss = 1.58659154\n",
            "Iteration 114, loss = 1.58307956\n",
            "Iteration 115, loss = 1.57956941\n",
            "Iteration 116, loss = 1.57603582\n",
            "Iteration 117, loss = 1.57247734\n",
            "Iteration 118, loss = 1.56889928\n",
            "Iteration 119, loss = 1.56531969\n",
            "Iteration 120, loss = 1.56175120\n",
            "Iteration 121, loss = 1.55821038\n",
            "Iteration 122, loss = 1.55466736\n",
            "Iteration 123, loss = 1.55115073\n",
            "Iteration 124, loss = 1.54762811\n",
            "Iteration 125, loss = 1.54409716\n",
            "Iteration 126, loss = 1.54056932\n",
            "Iteration 127, loss = 1.53704838\n",
            "Iteration 128, loss = 1.53353422\n",
            "Iteration 129, loss = 1.53000953\n",
            "Iteration 130, loss = 1.52647403\n",
            "Iteration 131, loss = 1.52292975\n",
            "Iteration 132, loss = 1.51937846\n",
            "Iteration 133, loss = 1.51581486\n",
            "Iteration 134, loss = 1.51224286\n",
            "Iteration 135, loss = 1.50865912\n",
            "Iteration 136, loss = 1.50506156\n",
            "Iteration 137, loss = 1.50145313\n",
            "Iteration 138, loss = 1.49784073\n",
            "Iteration 139, loss = 1.49424818\n",
            "Iteration 140, loss = 1.49063450\n",
            "Iteration 141, loss = 1.48702483\n",
            "Iteration 142, loss = 1.48340814\n",
            "Iteration 143, loss = 1.47979413\n",
            "Iteration 144, loss = 1.47617184\n",
            "Iteration 145, loss = 1.47253667\n",
            "Iteration 146, loss = 1.46889722\n",
            "Iteration 147, loss = 1.46525974\n",
            "Iteration 148, loss = 1.46160671\n",
            "Iteration 149, loss = 1.45795070\n",
            "Iteration 150, loss = 1.45429341\n",
            "Iteration 151, loss = 1.45062622\n",
            "Iteration 152, loss = 1.44695573\n",
            "Iteration 153, loss = 1.44327917\n",
            "Iteration 154, loss = 1.43960057\n",
            "Iteration 155, loss = 1.43591295\n",
            "Iteration 156, loss = 1.43222337\n",
            "Iteration 157, loss = 1.42852899\n",
            "Iteration 158, loss = 1.42483473\n",
            "Iteration 159, loss = 1.42113192\n",
            "Iteration 160, loss = 1.41742474\n",
            "Iteration 161, loss = 1.41371830\n",
            "Iteration 162, loss = 1.41000438\n",
            "Iteration 163, loss = 1.40628934\n",
            "Iteration 164, loss = 1.40257358\n",
            "Iteration 165, loss = 1.39885548\n",
            "Iteration 166, loss = 1.39513568\n",
            "Iteration 167, loss = 1.39141506\n",
            "Iteration 168, loss = 1.38769343\n",
            "Iteration 169, loss = 1.38397172\n",
            "Iteration 170, loss = 1.38025017\n",
            "Iteration 171, loss = 1.37653197\n",
            "Iteration 172, loss = 1.37281212\n",
            "Iteration 173, loss = 1.36909294\n",
            "Iteration 174, loss = 1.36537600\n",
            "Iteration 175, loss = 1.36166162\n",
            "Iteration 176, loss = 1.35795596\n",
            "Iteration 177, loss = 1.35424640\n",
            "Iteration 178, loss = 1.35055256\n",
            "Iteration 179, loss = 1.34686307\n",
            "Iteration 180, loss = 1.34317838\n",
            "Iteration 181, loss = 1.33949881\n",
            "Iteration 182, loss = 1.33582428\n",
            "Iteration 183, loss = 1.33215527\n",
            "Iteration 184, loss = 1.32849211\n",
            "Iteration 185, loss = 1.32483570\n",
            "Iteration 186, loss = 1.32118526\n",
            "Iteration 187, loss = 1.31754211\n",
            "Iteration 188, loss = 1.31390664\n",
            "Iteration 189, loss = 1.31027865\n",
            "Iteration 190, loss = 1.30665888\n",
            "Iteration 191, loss = 1.30304759\n",
            "Iteration 192, loss = 1.29944641\n",
            "Iteration 193, loss = 1.29585281\n",
            "Iteration 194, loss = 1.29226842\n",
            "Iteration 195, loss = 1.28869319\n",
            "Iteration 196, loss = 1.28512862\n",
            "Iteration 197, loss = 1.28157444\n",
            "Iteration 198, loss = 1.27803080\n",
            "Iteration 199, loss = 1.27449802\n",
            "Iteration 200, loss = 1.27097639\n",
            "Iteration 201, loss = 1.26746618\n",
            "Iteration 202, loss = 1.26396768\n",
            "Iteration 203, loss = 1.26048114\n",
            "Iteration 204, loss = 1.25700681\n",
            "Iteration 205, loss = 1.25354491\n",
            "Iteration 206, loss = 1.25009567\n",
            "Iteration 207, loss = 1.24665969\n",
            "Iteration 208, loss = 1.24323617\n",
            "Iteration 209, loss = 1.23982618\n",
            "Iteration 210, loss = 1.23642958\n",
            "Iteration 211, loss = 1.23304676\n",
            "Iteration 212, loss = 1.22967747\n",
            "Iteration 213, loss = 1.22632213\n",
            "Iteration 214, loss = 1.22298079\n",
            "Iteration 215, loss = 1.21965359\n",
            "Iteration 216, loss = 1.21634065\n",
            "Iteration 217, loss = 1.21304208\n",
            "Iteration 218, loss = 1.20975800\n",
            "Iteration 219, loss = 1.20648870\n",
            "Iteration 220, loss = 1.20323387\n",
            "Iteration 221, loss = 1.19999389\n",
            "Iteration 222, loss = 1.19677086\n",
            "Iteration 223, loss = 1.19357126\n",
            "Iteration 224, loss = 1.19038537\n",
            "Iteration 225, loss = 1.18721507\n",
            "Iteration 226, loss = 1.18406316\n",
            "Iteration 227, loss = 1.18093279\n",
            "Iteration 228, loss = 1.17781726\n",
            "Iteration 229, loss = 1.17472241\n",
            "Iteration 230, loss = 1.17164494\n",
            "Iteration 231, loss = 1.16858839\n",
            "Iteration 232, loss = 1.16554903\n",
            "Iteration 233, loss = 1.16252784\n",
            "Iteration 234, loss = 1.15953190\n",
            "Iteration 235, loss = 1.15655968\n",
            "Iteration 236, loss = 1.15360619\n",
            "Iteration 237, loss = 1.15067122\n",
            "Iteration 238, loss = 1.14775459\n",
            "Iteration 239, loss = 1.14485677\n",
            "Iteration 240, loss = 1.14197987\n",
            "Iteration 241, loss = 1.13911866\n",
            "Iteration 242, loss = 1.13627761\n",
            "Iteration 243, loss = 1.13345534\n",
            "Iteration 244, loss = 1.13065256\n",
            "Iteration 245, loss = 1.12786991\n",
            "Iteration 246, loss = 1.12510942\n",
            "Iteration 247, loss = 1.12236756\n",
            "Iteration 248, loss = 1.11964424\n",
            "Iteration 249, loss = 1.11693997\n",
            "Iteration 250, loss = 1.11425468\n",
            "Iteration 251, loss = 1.11158832\n",
            "Iteration 252, loss = 1.10894075\n",
            "Iteration 253, loss = 1.10631176\n",
            "Iteration 254, loss = 1.10370096\n",
            "Iteration 255, loss = 1.10110809\n",
            "Iteration 256, loss = 1.09854732\n",
            "Iteration 257, loss = 1.09599305\n",
            "Iteration 258, loss = 1.09346743\n",
            "Iteration 259, loss = 1.09096334\n",
            "Iteration 260, loss = 1.08848091\n",
            "Iteration 261, loss = 1.08601989\n",
            "Iteration 262, loss = 1.08357971\n",
            "Iteration 263, loss = 1.08115961\n",
            "Iteration 264, loss = 1.07875876\n",
            "Iteration 265, loss = 1.07637636\n",
            "Iteration 266, loss = 1.07402252\n",
            "Iteration 267, loss = 1.07168417\n",
            "Iteration 268, loss = 1.06936584\n",
            "Iteration 269, loss = 1.06706839\n",
            "Iteration 270, loss = 1.06479188\n",
            "Iteration 271, loss = 1.06253640\n",
            "Iteration 272, loss = 1.06030143\n",
            "Iteration 273, loss = 1.05808652\n",
            "Iteration 274, loss = 1.05589098\n",
            "Iteration 275, loss = 1.05371385\n",
            "Iteration 276, loss = 1.05155460\n",
            "Iteration 277, loss = 1.04941244\n",
            "Iteration 278, loss = 1.04728682\n",
            "Iteration 279, loss = 1.04517726\n",
            "Iteration 280, loss = 1.04308404\n",
            "Iteration 281, loss = 1.04100659\n",
            "Iteration 282, loss = 1.03894468\n",
            "Iteration 283, loss = 1.03689835\n",
            "Iteration 284, loss = 1.03486915\n",
            "Iteration 285, loss = 1.03285473\n",
            "Iteration 286, loss = 1.03085519\n",
            "Iteration 287, loss = 1.02887048\n",
            "Iteration 288, loss = 1.02690033\n",
            "Iteration 289, loss = 1.02494468\n",
            "Iteration 290, loss = 1.02300436\n",
            "Iteration 291, loss = 1.02107850\n",
            "Iteration 292, loss = 1.01916670\n",
            "Iteration 293, loss = 1.01726880\n",
            "Iteration 294, loss = 1.01538462\n",
            "Iteration 295, loss = 1.01351402\n",
            "Iteration 296, loss = 1.01166278\n",
            "Iteration 297, loss = 1.00983835\n",
            "Iteration 298, loss = 1.00802686\n",
            "Iteration 299, loss = 1.00622934\n",
            "Iteration 300, loss = 1.00444700\n",
            "Iteration 301, loss = 1.00268118\n",
            "Iteration 302, loss = 1.00093192\n",
            "Iteration 303, loss = 0.99919864\n",
            "Iteration 304, loss = 0.99748081\n",
            "Iteration 305, loss = 0.99578291\n",
            "Iteration 306, loss = 0.99410112\n",
            "Iteration 307, loss = 0.99243583\n",
            "Iteration 308, loss = 0.99078732\n",
            "Iteration 309, loss = 0.98915533\n",
            "Iteration 310, loss = 0.98753917\n",
            "Iteration 311, loss = 0.98593789\n",
            "Iteration 312, loss = 0.98435046\n",
            "Iteration 313, loss = 0.98277591\n",
            "Iteration 314, loss = 0.98121342\n",
            "Iteration 315, loss = 0.97966240\n",
            "Iteration 316, loss = 0.97812245\n",
            "Iteration 317, loss = 0.97659364\n",
            "Iteration 318, loss = 0.97507633\n",
            "Iteration 319, loss = 0.97356990\n",
            "Iteration 320, loss = 0.97208877\n",
            "Iteration 321, loss = 0.97062078\n",
            "Iteration 322, loss = 0.96915816\n",
            "Iteration 323, loss = 0.96771095\n",
            "Iteration 324, loss = 0.96628216\n",
            "Iteration 325, loss = 0.96486367\n",
            "Iteration 326, loss = 0.96346308\n",
            "Iteration 327, loss = 0.96206976\n",
            "Iteration 328, loss = 0.96068894\n",
            "Iteration 329, loss = 0.95932469\n",
            "Iteration 330, loss = 0.95796985\n",
            "Iteration 331, loss = 0.95662782\n",
            "Iteration 332, loss = 0.95529631\n",
            "Iteration 333, loss = 0.95397515\n",
            "Iteration 334, loss = 0.95266410\n",
            "Iteration 335, loss = 0.95136282\n",
            "Iteration 336, loss = 0.95007104\n",
            "Iteration 337, loss = 0.94878952\n",
            "Iteration 338, loss = 0.94751704\n",
            "Iteration 339, loss = 0.94625363\n",
            "Iteration 340, loss = 0.94499919\n",
            "Iteration 341, loss = 0.94375318\n",
            "Iteration 342, loss = 0.94251553\n",
            "Iteration 343, loss = 0.94128638\n",
            "Iteration 344, loss = 0.94006533\n",
            "Iteration 345, loss = 0.93885234\n",
            "Iteration 346, loss = 0.93764755\n",
            "Iteration 347, loss = 0.93645056\n",
            "Iteration 348, loss = 0.93526129\n",
            "Iteration 349, loss = 0.93408001\n",
            "Iteration 350, loss = 0.93290628\n",
            "Iteration 351, loss = 0.93173997\n",
            "Iteration 352, loss = 0.93058142\n",
            "Iteration 353, loss = 0.92942961\n",
            "Iteration 354, loss = 0.92828532\n",
            "Iteration 355, loss = 0.92714796\n",
            "Iteration 356, loss = 0.92602388\n",
            "Iteration 357, loss = 0.92490414\n",
            "Iteration 358, loss = 0.92379057\n",
            "Iteration 359, loss = 0.92268391\n",
            "Iteration 360, loss = 0.92158537\n",
            "Iteration 361, loss = 0.92049373\n",
            "Iteration 362, loss = 0.91941079\n",
            "Iteration 363, loss = 0.91833596\n",
            "Iteration 364, loss = 0.91726879\n",
            "Iteration 365, loss = 0.91620948\n",
            "Iteration 366, loss = 0.91515751\n",
            "Iteration 367, loss = 0.91411180\n",
            "Iteration 368, loss = 0.91307206\n",
            "Iteration 369, loss = 0.91203807\n",
            "Iteration 370, loss = 0.91100975\n",
            "Iteration 371, loss = 0.90998718\n",
            "Iteration 372, loss = 0.90897029\n",
            "Iteration 373, loss = 0.90795903\n",
            "Iteration 374, loss = 0.90695366\n",
            "Iteration 375, loss = 0.90595383\n",
            "Iteration 376, loss = 0.90495965\n",
            "Iteration 377, loss = 0.90397096\n",
            "Iteration 378, loss = 0.90298786\n",
            "Iteration 379, loss = 0.90201008\n",
            "Iteration 380, loss = 0.90103742\n",
            "Iteration 381, loss = 0.90006982\n",
            "Iteration 382, loss = 0.89910768\n",
            "Iteration 383, loss = 0.89814987\n",
            "Iteration 384, loss = 0.89719720\n",
            "Iteration 385, loss = 0.89624959\n",
            "Iteration 386, loss = 0.89530655\n",
            "Iteration 387, loss = 0.89436819\n",
            "Iteration 388, loss = 0.89343445\n",
            "Iteration 389, loss = 0.89250572\n",
            "Iteration 390, loss = 0.89158104\n",
            "Iteration 391, loss = 0.89066105\n",
            "Iteration 392, loss = 0.88974584\n",
            "Iteration 393, loss = 0.88883476\n",
            "Iteration 394, loss = 0.88792801\n",
            "Iteration 395, loss = 0.88702552\n",
            "Iteration 396, loss = 0.88612731\n",
            "Iteration 397, loss = 0.88523350\n",
            "Iteration 398, loss = 0.88434367\n",
            "Iteration 399, loss = 0.88345790\n",
            "Iteration 400, loss = 0.88257641\n",
            "Iteration 401, loss = 0.88169875\n",
            "Iteration 402, loss = 0.88082508\n",
            "Iteration 403, loss = 0.87995530\n",
            "Iteration 404, loss = 0.87908967\n",
            "Iteration 405, loss = 0.87822762\n",
            "Iteration 406, loss = 0.87736945\n",
            "Iteration 407, loss = 0.87651501\n",
            "Iteration 408, loss = 0.87566446\n",
            "Iteration 409, loss = 0.87481752\n",
            "Iteration 410, loss = 0.87397421\n",
            "Iteration 411, loss = 0.87313449\n",
            "Iteration 412, loss = 0.87229833\n",
            "Iteration 413, loss = 0.87146615\n",
            "Iteration 414, loss = 0.87063685\n",
            "Iteration 415, loss = 0.86981130\n",
            "Iteration 416, loss = 0.86898918\n",
            "Iteration 417, loss = 0.86817072\n",
            "Iteration 418, loss = 0.86735546\n",
            "Iteration 419, loss = 0.86654351\n",
            "Iteration 420, loss = 0.86573486\n",
            "Iteration 421, loss = 0.86492956\n",
            "Iteration 422, loss = 0.86412772\n",
            "Iteration 423, loss = 0.86332903\n",
            "Iteration 424, loss = 0.86253372\n",
            "Iteration 425, loss = 0.86174142\n",
            "Iteration 426, loss = 0.86095221\n",
            "Iteration 427, loss = 0.86016631\n",
            "Iteration 428, loss = 0.85938338\n",
            "Iteration 429, loss = 0.85860375\n",
            "Iteration 430, loss = 0.85782743\n",
            "Iteration 431, loss = 0.85705442\n",
            "Iteration 432, loss = 0.85628405\n",
            "Iteration 433, loss = 0.85551675\n",
            "Iteration 434, loss = 0.85475235\n",
            "Iteration 435, loss = 0.85399121\n",
            "Iteration 436, loss = 0.85323254\n",
            "Iteration 437, loss = 0.85247689\n",
            "Iteration 438, loss = 0.85172405\n",
            "Iteration 439, loss = 0.85097427\n",
            "Iteration 440, loss = 0.85022707\n",
            "Iteration 441, loss = 0.84948270\n",
            "Iteration 442, loss = 0.84874105\n",
            "Iteration 443, loss = 0.84800211\n",
            "Iteration 444, loss = 0.84726631\n",
            "Iteration 445, loss = 0.84653261\n",
            "Iteration 446, loss = 0.84580183\n",
            "Iteration 447, loss = 0.84507368\n",
            "Iteration 448, loss = 0.84435313\n",
            "Iteration 449, loss = 0.84362877\n",
            "Iteration 450, loss = 0.84291105\n",
            "Iteration 451, loss = 0.84219606\n",
            "Iteration 452, loss = 0.84148536\n",
            "Iteration 453, loss = 0.84078031\n",
            "Iteration 454, loss = 0.84007229\n",
            "Iteration 455, loss = 0.83937218\n",
            "Iteration 456, loss = 0.83867071\n",
            "Iteration 457, loss = 0.83797448\n",
            "Iteration 458, loss = 0.83728098\n",
            "Iteration 459, loss = 0.83659007\n",
            "Iteration 460, loss = 0.83590238\n",
            "Iteration 461, loss = 0.83521704\n",
            "Iteration 462, loss = 0.83453393\n",
            "Iteration 463, loss = 0.83385408\n",
            "Iteration 464, loss = 0.83317513\n",
            "Iteration 465, loss = 0.83250053\n",
            "Iteration 466, loss = 0.83182699\n",
            "Iteration 467, loss = 0.83115640\n",
            "Iteration 468, loss = 0.83048801\n",
            "Iteration 469, loss = 0.82982140\n",
            "Iteration 470, loss = 0.82915780\n",
            "Iteration 471, loss = 0.82849578\n",
            "Iteration 472, loss = 0.82783585\n",
            "Iteration 473, loss = 0.82717796\n",
            "Iteration 474, loss = 0.82652446\n",
            "Iteration 475, loss = 0.82586993\n",
            "Iteration 476, loss = 0.82521876\n",
            "Iteration 477, loss = 0.82456990\n",
            "Iteration 478, loss = 0.82392275\n",
            "Iteration 479, loss = 0.82327769\n",
            "Iteration 480, loss = 0.82263551\n",
            "Iteration 481, loss = 0.82199431\n",
            "Iteration 482, loss = 0.82135566\n",
            "Iteration 483, loss = 0.82071890\n",
            "Iteration 484, loss = 0.82008398\n",
            "Iteration 485, loss = 0.81945096\n",
            "Iteration 486, loss = 0.81882007\n",
            "Iteration 487, loss = 0.81819145\n",
            "Iteration 488, loss = 0.81756475\n",
            "Iteration 489, loss = 0.81693954\n",
            "Iteration 490, loss = 0.81631624\n",
            "Iteration 491, loss = 0.81569476\n",
            "Iteration 492, loss = 0.81507526\n",
            "Iteration 493, loss = 0.81445848\n",
            "Iteration 494, loss = 0.81384287\n",
            "Iteration 495, loss = 0.81322893\n",
            "Iteration 496, loss = 0.81261671\n",
            "Iteration 497, loss = 0.81200624\n",
            "Iteration 498, loss = 0.81139809\n",
            "Iteration 499, loss = 0.81079159\n",
            "Iteration 500, loss = 0.81018691\n",
            "Iteration 501, loss = 0.80958368\n",
            "Iteration 502, loss = 0.80898304\n",
            "Iteration 503, loss = 0.80838482\n",
            "Iteration 504, loss = 0.80778774\n",
            "Iteration 505, loss = 0.80719255\n",
            "Iteration 506, loss = 0.80659898\n",
            "Iteration 507, loss = 0.80600706\n",
            "Iteration 508, loss = 0.80541682\n",
            "Iteration 509, loss = 0.80482910\n",
            "Iteration 510, loss = 0.80424199\n",
            "Iteration 511, loss = 0.80365693\n",
            "Iteration 512, loss = 0.80307341\n",
            "Iteration 513, loss = 0.80249148\n",
            "Iteration 514, loss = 0.80191113\n",
            "Iteration 515, loss = 0.80133320\n",
            "Iteration 516, loss = 0.80075578\n",
            "Iteration 517, loss = 0.80018035\n",
            "Iteration 518, loss = 0.79960639\n",
            "Iteration 519, loss = 0.79903392\n",
            "Iteration 520, loss = 0.79846297\n",
            "Iteration 521, loss = 0.79789354\n",
            "Iteration 522, loss = 0.79732624\n",
            "Iteration 523, loss = 0.79675963\n",
            "Iteration 524, loss = 0.79619483\n",
            "Iteration 525, loss = 0.79563142\n",
            "Iteration 526, loss = 0.79507013\n",
            "Iteration 527, loss = 0.79450902\n",
            "Iteration 528, loss = 0.79395018\n",
            "Iteration 529, loss = 0.79339344\n",
            "Iteration 530, loss = 0.79283688\n",
            "Iteration 531, loss = 0.79228228\n",
            "Iteration 532, loss = 0.79172903\n",
            "Iteration 533, loss = 0.79117710\n",
            "Iteration 534, loss = 0.79062722\n",
            "Iteration 535, loss = 0.79007788\n",
            "Iteration 536, loss = 0.78953030\n",
            "Iteration 537, loss = 0.78898430\n",
            "Iteration 538, loss = 0.78843919\n",
            "Iteration 539, loss = 0.78789585\n",
            "Iteration 540, loss = 0.78735355\n",
            "Iteration 541, loss = 0.78681279\n",
            "Iteration 542, loss = 0.78627321\n",
            "Iteration 543, loss = 0.78573489\n",
            "Iteration 544, loss = 0.78519791\n",
            "Iteration 545, loss = 0.78466238\n",
            "Iteration 546, loss = 0.78412794\n",
            "Iteration 547, loss = 0.78359487\n",
            "Iteration 548, loss = 0.78306312\n",
            "Iteration 549, loss = 0.78253243\n",
            "Iteration 550, loss = 0.78200381\n",
            "Iteration 551, loss = 0.78147525\n",
            "Iteration 552, loss = 0.78094841\n",
            "Iteration 553, loss = 0.78042286\n",
            "Iteration 554, loss = 0.77989853\n",
            "Iteration 555, loss = 0.77937517\n",
            "Iteration 556, loss = 0.77885323\n",
            "Iteration 557, loss = 0.77833213\n",
            "Iteration 558, loss = 0.77781246\n",
            "Iteration 559, loss = 0.77729384\n",
            "Iteration 560, loss = 0.77677632\n",
            "Iteration 561, loss = 0.77626026\n",
            "Iteration 562, loss = 0.77574496\n",
            "Iteration 563, loss = 0.77523120\n",
            "Iteration 564, loss = 0.77471819\n",
            "Iteration 565, loss = 0.77420651\n",
            "Iteration 566, loss = 0.77369578\n",
            "Iteration 567, loss = 0.77318647\n",
            "Iteration 568, loss = 0.77267783\n",
            "Iteration 569, loss = 0.77217067\n",
            "Iteration 570, loss = 0.77166425\n",
            "Iteration 571, loss = 0.77115908\n",
            "Iteration 572, loss = 0.77065498\n",
            "Iteration 573, loss = 0.77015173\n",
            "Iteration 574, loss = 0.76964992\n",
            "Iteration 575, loss = 0.76914862\n",
            "Iteration 576, loss = 0.76864891\n",
            "Iteration 577, loss = 0.76814984\n",
            "Iteration 578, loss = 0.76765167\n",
            "Iteration 579, loss = 0.76715490\n",
            "Iteration 580, loss = 0.76665864\n",
            "Iteration 581, loss = 0.76616390\n",
            "Iteration 582, loss = 0.76566981\n",
            "Iteration 583, loss = 0.76517661\n",
            "Iteration 584, loss = 0.76468462\n",
            "Iteration 585, loss = 0.76419336\n",
            "Iteration 586, loss = 0.76370317\n",
            "Iteration 587, loss = 0.76321405\n",
            "Iteration 588, loss = 0.76272567\n",
            "Iteration 589, loss = 0.76223816\n",
            "Iteration 590, loss = 0.76175200\n",
            "Iteration 591, loss = 0.76126616\n",
            "Iteration 592, loss = 0.76078159\n",
            "Iteration 593, loss = 0.76029793\n",
            "Iteration 594, loss = 0.75981505\n",
            "Iteration 595, loss = 0.75933300\n",
            "Iteration 596, loss = 0.75885186\n",
            "Iteration 597, loss = 0.75837177\n",
            "Iteration 598, loss = 0.75789237\n",
            "Iteration 599, loss = 0.75741379\n",
            "Iteration 600, loss = 0.75693627\n",
            "Iteration 601, loss = 0.75645940\n",
            "Iteration 602, loss = 0.75598341\n",
            "Iteration 603, loss = 0.75550820\n",
            "Iteration 604, loss = 0.75503415\n",
            "Iteration 605, loss = 0.75456048\n",
            "Iteration 606, loss = 0.75408778\n",
            "Iteration 607, loss = 0.75361586\n",
            "Iteration 608, loss = 0.75314490\n",
            "Iteration 609, loss = 0.75267465\n",
            "Iteration 610, loss = 0.75220516\n",
            "Iteration 611, loss = 0.75173643\n",
            "Iteration 612, loss = 0.75126844\n",
            "Iteration 613, loss = 0.75080139\n",
            "Iteration 614, loss = 0.75033500\n",
            "Iteration 615, loss = 0.74986936\n",
            "Iteration 616, loss = 0.74940445\n",
            "Iteration 617, loss = 0.74894027\n",
            "Iteration 618, loss = 0.74847720\n",
            "Iteration 619, loss = 0.74801430\n",
            "Iteration 620, loss = 0.74755236\n",
            "Iteration 621, loss = 0.74709110\n",
            "Iteration 622, loss = 0.74663054\n",
            "Iteration 623, loss = 0.74617066\n",
            "Iteration 624, loss = 0.74571177\n",
            "Iteration 625, loss = 0.74525318\n",
            "Iteration 626, loss = 0.74479543\n",
            "Iteration 627, loss = 0.74433833\n",
            "Iteration 628, loss = 0.74388190\n",
            "Iteration 629, loss = 0.74342611\n",
            "Iteration 630, loss = 0.74297097\n",
            "Iteration 631, loss = 0.74251649\n",
            "Iteration 632, loss = 0.74206284\n",
            "Iteration 633, loss = 0.74160960\n",
            "Iteration 634, loss = 0.74115709\n",
            "Iteration 635, loss = 0.74070519\n",
            "Iteration 636, loss = 0.74025391\n",
            "Iteration 637, loss = 0.73980324\n",
            "Iteration 638, loss = 0.73935317\n",
            "Iteration 639, loss = 0.73890371\n",
            "Iteration 640, loss = 0.73845485\n",
            "Iteration 641, loss = 0.73800658\n",
            "Iteration 642, loss = 0.73755890\n",
            "Iteration 643, loss = 0.73711180\n",
            "Iteration 644, loss = 0.73666529\n",
            "Iteration 645, loss = 0.73621935\n",
            "Iteration 646, loss = 0.73577397\n",
            "Iteration 647, loss = 0.73532916\n",
            "Iteration 648, loss = 0.73488504\n",
            "Iteration 649, loss = 0.73444229\n",
            "Iteration 650, loss = 0.73400038\n",
            "Iteration 651, loss = 0.73355905\n",
            "Iteration 652, loss = 0.73311829\n",
            "Iteration 653, loss = 0.73267807\n",
            "Iteration 654, loss = 0.73223840\n",
            "Iteration 655, loss = 0.73179926\n",
            "Iteration 656, loss = 0.73136065\n",
            "Iteration 657, loss = 0.73092257\n",
            "Iteration 658, loss = 0.73048500\n",
            "Iteration 659, loss = 0.73004810\n",
            "Iteration 660, loss = 0.72961203\n",
            "Iteration 661, loss = 0.72917640\n",
            "Iteration 662, loss = 0.72874121\n",
            "Iteration 663, loss = 0.72830646\n",
            "Iteration 664, loss = 0.72787215\n",
            "Iteration 665, loss = 0.72743829\n",
            "Iteration 666, loss = 0.72700489\n",
            "Iteration 667, loss = 0.72657193\n",
            "Iteration 668, loss = 0.72613939\n",
            "Iteration 669, loss = 0.72570727\n",
            "Iteration 670, loss = 0.72527558\n",
            "Iteration 671, loss = 0.72484433\n",
            "Iteration 672, loss = 0.72441350\n",
            "Iteration 673, loss = 0.72398308\n",
            "Iteration 674, loss = 0.72355313\n",
            "Iteration 675, loss = 0.72312373\n",
            "Iteration 676, loss = 0.72269471\n",
            "Iteration 677, loss = 0.72226608\n",
            "Iteration 678, loss = 0.72183782\n",
            "Iteration 679, loss = 0.72140994\n",
            "Iteration 680, loss = 0.72098248\n",
            "Iteration 681, loss = 0.72055539\n",
            "Iteration 682, loss = 0.72012871\n",
            "Iteration 683, loss = 0.71970237\n",
            "Iteration 684, loss = 0.71927642\n",
            "Iteration 685, loss = 0.71885080\n",
            "Iteration 686, loss = 0.71842557\n",
            "Iteration 687, loss = 0.71800066\n",
            "Iteration 688, loss = 0.71757610\n",
            "Iteration 689, loss = 0.71715190\n",
            "Iteration 690, loss = 0.71672799\n",
            "Iteration 691, loss = 0.71630446\n",
            "Iteration 692, loss = 0.71588119\n",
            "Iteration 693, loss = 0.71545830\n",
            "Iteration 694, loss = 0.71503567\n",
            "Iteration 695, loss = 0.71461333\n",
            "Iteration 696, loss = 0.71419135\n",
            "Iteration 697, loss = 0.71376961\n",
            "Iteration 698, loss = 0.71334870\n",
            "Iteration 699, loss = 0.71292742\n",
            "Iteration 700, loss = 0.71250671\n",
            "Iteration 701, loss = 0.71208619\n",
            "Iteration 702, loss = 0.71166593\n",
            "Iteration 703, loss = 0.71124634\n",
            "Iteration 704, loss = 0.71082675\n",
            "Iteration 705, loss = 0.71040730\n",
            "Iteration 706, loss = 0.70998836\n",
            "Iteration 707, loss = 0.70956978\n",
            "Iteration 708, loss = 0.70915098\n",
            "Iteration 709, loss = 0.70873323\n",
            "Iteration 710, loss = 0.70831473\n",
            "Iteration 711, loss = 0.70789739\n",
            "Iteration 712, loss = 0.70747950\n",
            "Iteration 713, loss = 0.70706258\n",
            "Iteration 714, loss = 0.70664524\n",
            "Iteration 715, loss = 0.70623018\n",
            "Iteration 716, loss = 0.70581341\n",
            "Iteration 717, loss = 0.70539643\n",
            "Iteration 718, loss = 0.70498052\n",
            "Iteration 719, loss = 0.70456542\n",
            "Iteration 720, loss = 0.70414971\n",
            "Iteration 721, loss = 0.70373434\n",
            "Iteration 722, loss = 0.70331969\n",
            "Iteration 723, loss = 0.70290440\n",
            "Iteration 724, loss = 0.70248968\n",
            "Iteration 725, loss = 0.70207510\n",
            "Iteration 726, loss = 0.70166119\n",
            "Iteration 727, loss = 0.70124719\n",
            "Iteration 728, loss = 0.70083334\n",
            "Iteration 729, loss = 0.70041916\n",
            "Iteration 730, loss = 0.70000557\n",
            "Iteration 731, loss = 0.69959280\n",
            "Iteration 732, loss = 0.69918131\n",
            "Iteration 733, loss = 0.69876768\n",
            "Iteration 734, loss = 0.69835377\n",
            "Iteration 735, loss = 0.69794211\n",
            "Iteration 736, loss = 0.69752984\n",
            "Iteration 737, loss = 0.69711753\n",
            "Iteration 738, loss = 0.69670619\n",
            "Iteration 739, loss = 0.69629428\n",
            "Iteration 740, loss = 0.69588230\n",
            "Iteration 741, loss = 0.69547135\n",
            "Iteration 742, loss = 0.69505988\n",
            "Iteration 743, loss = 0.69464845\n",
            "Iteration 744, loss = 0.69423903\n",
            "Iteration 745, loss = 0.69382708\n",
            "Iteration 746, loss = 0.69341613\n",
            "Iteration 747, loss = 0.69300561\n",
            "Iteration 748, loss = 0.69259536\n",
            "Iteration 749, loss = 0.69218522\n",
            "Iteration 750, loss = 0.69177475\n",
            "Iteration 751, loss = 0.69136482\n",
            "Iteration 752, loss = 0.69095437\n",
            "Iteration 753, loss = 0.69054449\n",
            "Iteration 754, loss = 0.69013421\n",
            "Iteration 755, loss = 0.68972425\n",
            "Iteration 756, loss = 0.68931818\n",
            "Iteration 757, loss = 0.68890731\n",
            "Iteration 758, loss = 0.68849866\n",
            "Iteration 759, loss = 0.68809014\n",
            "Iteration 760, loss = 0.68768327\n",
            "Iteration 761, loss = 0.68727646\n",
            "Iteration 762, loss = 0.68687101\n",
            "Iteration 763, loss = 0.68646397\n",
            "Iteration 764, loss = 0.68605760\n",
            "Iteration 765, loss = 0.68565145\n",
            "Iteration 766, loss = 0.68524591\n",
            "Iteration 767, loss = 0.68483996\n",
            "Iteration 768, loss = 0.68443400\n",
            "Iteration 769, loss = 0.68402900\n",
            "Iteration 770, loss = 0.68362264\n",
            "Iteration 771, loss = 0.68321740\n",
            "Iteration 772, loss = 0.68281248\n",
            "Iteration 773, loss = 0.68240711\n",
            "Iteration 774, loss = 0.68200183\n",
            "Iteration 775, loss = 0.68159651\n",
            "Iteration 776, loss = 0.68119109\n",
            "Iteration 777, loss = 0.68078573\n",
            "Iteration 778, loss = 0.68038145\n",
            "Iteration 779, loss = 0.67997569\n",
            "Iteration 780, loss = 0.67957044\n",
            "Iteration 781, loss = 0.67916521\n",
            "Iteration 782, loss = 0.67876045\n",
            "Iteration 783, loss = 0.67835539\n",
            "Iteration 784, loss = 0.67795036\n",
            "Iteration 785, loss = 0.67754526\n",
            "Iteration 786, loss = 0.67714041\n",
            "Iteration 787, loss = 0.67673541\n",
            "Iteration 788, loss = 0.67633030\n",
            "Iteration 789, loss = 0.67592506\n",
            "Iteration 790, loss = 0.67551971\n",
            "Iteration 791, loss = 0.67511429\n",
            "Iteration 792, loss = 0.67470885\n",
            "Iteration 793, loss = 0.67430349\n",
            "Iteration 794, loss = 0.67389832\n",
            "Iteration 795, loss = 0.67349281\n",
            "Iteration 796, loss = 0.67308725\n",
            "Iteration 797, loss = 0.67268159\n",
            "Iteration 798, loss = 0.67227586\n",
            "Iteration 799, loss = 0.67187047\n",
            "Iteration 800, loss = 0.67146441\n",
            "Iteration 801, loss = 0.67105856\n",
            "Iteration 802, loss = 0.67065264\n",
            "Iteration 803, loss = 0.67024671\n",
            "Iteration 804, loss = 0.66984075\n",
            "Iteration 805, loss = 0.66943468\n",
            "Iteration 806, loss = 0.66902841\n",
            "Iteration 807, loss = 0.66862200\n",
            "Iteration 808, loss = 0.66821544\n",
            "Iteration 809, loss = 0.66780876\n",
            "Iteration 810, loss = 0.66740198\n",
            "Iteration 811, loss = 0.66699512\n",
            "Iteration 812, loss = 0.66658825\n",
            "Iteration 813, loss = 0.66618128\n",
            "Iteration 814, loss = 0.66577420\n",
            "Iteration 815, loss = 0.66536700\n",
            "Iteration 816, loss = 0.66495967\n",
            "Iteration 817, loss = 0.66455220\n",
            "Iteration 818, loss = 0.66414459\n",
            "Iteration 819, loss = 0.66373683\n",
            "Iteration 820, loss = 0.66332893\n",
            "Iteration 821, loss = 0.66292090\n",
            "Iteration 822, loss = 0.66251272\n",
            "Iteration 823, loss = 0.66210439\n",
            "Iteration 824, loss = 0.66169590\n",
            "Iteration 825, loss = 0.66128725\n",
            "Iteration 826, loss = 0.66087843\n",
            "Iteration 827, loss = 0.66046945\n",
            "Iteration 828, loss = 0.66006029\n",
            "Iteration 829, loss = 0.65965096\n",
            "Iteration 830, loss = 0.65924144\n",
            "Iteration 831, loss = 0.65883174\n",
            "Iteration 832, loss = 0.65842186\n",
            "Iteration 833, loss = 0.65801179\n",
            "Iteration 834, loss = 0.65760153\n",
            "Iteration 835, loss = 0.65719106\n",
            "Iteration 836, loss = 0.65678040\n",
            "Iteration 837, loss = 0.65636953\n",
            "Iteration 838, loss = 0.65595844\n",
            "Iteration 839, loss = 0.65554715\n",
            "Iteration 840, loss = 0.65513563\n",
            "Iteration 841, loss = 0.65472390\n",
            "Iteration 842, loss = 0.65431194\n",
            "Iteration 843, loss = 0.65389975\n",
            "Iteration 844, loss = 0.65348733\n",
            "Iteration 845, loss = 0.65307468\n",
            "Iteration 846, loss = 0.65266179\n",
            "Iteration 847, loss = 0.65224866\n",
            "Iteration 848, loss = 0.65183528\n",
            "Iteration 849, loss = 0.65142166\n",
            "Iteration 850, loss = 0.65100778\n",
            "Iteration 851, loss = 0.65059364\n",
            "Iteration 852, loss = 0.65017925\n",
            "Iteration 853, loss = 0.64976459\n",
            "Iteration 854, loss = 0.64934966\n",
            "Iteration 855, loss = 0.64893447\n",
            "Iteration 856, loss = 0.64851900\n",
            "Iteration 857, loss = 0.64810325\n",
            "Iteration 858, loss = 0.64768723\n",
            "Iteration 859, loss = 0.64727092\n",
            "Iteration 860, loss = 0.64685432\n",
            "Iteration 861, loss = 0.64643743\n",
            "Iteration 862, loss = 0.64602025\n",
            "Iteration 863, loss = 0.64560277\n",
            "Iteration 864, loss = 0.64518499\n",
            "Iteration 865, loss = 0.64476691\n",
            "Iteration 866, loss = 0.64434852\n",
            "Iteration 867, loss = 0.64392982\n",
            "Iteration 868, loss = 0.64351081\n",
            "Iteration 869, loss = 0.64309148\n",
            "Iteration 870, loss = 0.64267183\n",
            "Iteration 871, loss = 0.64225186\n",
            "Iteration 872, loss = 0.64183156\n",
            "Iteration 873, loss = 0.64141093\n",
            "Iteration 874, loss = 0.64098997\n",
            "Iteration 875, loss = 0.64056867\n",
            "Iteration 876, loss = 0.64014704\n",
            "Iteration 877, loss = 0.63972506\n",
            "Iteration 878, loss = 0.63930274\n",
            "Iteration 879, loss = 0.63888007\n",
            "Iteration 880, loss = 0.63845705\n",
            "Iteration 881, loss = 0.63803367\n",
            "Iteration 882, loss = 0.63760994\n",
            "Iteration 883, loss = 0.63718585\n",
            "Iteration 884, loss = 0.63676139\n",
            "Iteration 885, loss = 0.63633657\n",
            "Iteration 886, loss = 0.63591138\n",
            "Iteration 887, loss = 0.63548581\n",
            "Iteration 888, loss = 0.63505988\n",
            "Iteration 889, loss = 0.63463356\n",
            "Iteration 890, loss = 0.63420686\n",
            "Iteration 891, loss = 0.63377978\n",
            "Iteration 892, loss = 0.63335231\n",
            "Iteration 893, loss = 0.63292445\n",
            "Iteration 894, loss = 0.63249620\n",
            "Iteration 895, loss = 0.63206755\n",
            "Iteration 896, loss = 0.63163851\n",
            "Iteration 897, loss = 0.63120907\n",
            "Iteration 898, loss = 0.63077922\n",
            "Iteration 899, loss = 0.63034896\n",
            "Iteration 900, loss = 0.62991830\n",
            "Iteration 901, loss = 0.62948962\n",
            "Iteration 902, loss = 0.62905866\n",
            "Iteration 903, loss = 0.62862663\n",
            "Iteration 904, loss = 0.62819432\n",
            "Iteration 905, loss = 0.62776240\n",
            "Iteration 906, loss = 0.62732991\n",
            "Iteration 907, loss = 0.62689772\n",
            "Iteration 908, loss = 0.62646418\n",
            "Iteration 909, loss = 0.62603056\n",
            "Iteration 910, loss = 0.62559706\n",
            "Iteration 911, loss = 0.62516257\n",
            "Iteration 912, loss = 0.62472751\n",
            "Iteration 913, loss = 0.62429290\n",
            "Iteration 914, loss = 0.62385675\n",
            "Iteration 915, loss = 0.62342124\n",
            "Iteration 916, loss = 0.62298470\n",
            "Iteration 917, loss = 0.62254778\n",
            "Iteration 918, loss = 0.62211094\n",
            "Iteration 919, loss = 0.62167320\n",
            "Iteration 920, loss = 0.62123500\n",
            "Iteration 921, loss = 0.62079715\n",
            "Iteration 922, loss = 0.62035785\n",
            "Iteration 923, loss = 0.61991874\n",
            "Iteration 924, loss = 0.61947925\n",
            "Iteration 925, loss = 0.61903895\n",
            "Iteration 926, loss = 0.61859847\n",
            "Iteration 927, loss = 0.61815753\n",
            "Iteration 928, loss = 0.61771589\n",
            "Iteration 929, loss = 0.61727456\n",
            "Iteration 930, loss = 0.61683200\n",
            "Iteration 931, loss = 0.61638944\n",
            "Iteration 932, loss = 0.61594604\n",
            "Iteration 933, loss = 0.61550277\n",
            "Iteration 934, loss = 0.61505845\n",
            "Iteration 935, loss = 0.61461417\n",
            "Iteration 936, loss = 0.61416917\n",
            "Iteration 937, loss = 0.61372374\n",
            "Iteration 938, loss = 0.61327811\n",
            "Iteration 939, loss = 0.61283150\n",
            "Iteration 940, loss = 0.61238523\n",
            "Iteration 941, loss = 0.61193754\n",
            "Iteration 942, loss = 0.61149064\n",
            "Iteration 943, loss = 0.61104222\n",
            "Iteration 944, loss = 0.61059332\n",
            "Iteration 945, loss = 0.61014446\n",
            "Iteration 946, loss = 0.60969488\n",
            "Iteration 947, loss = 0.60924495\n",
            "Iteration 948, loss = 0.60879444\n",
            "Iteration 949, loss = 0.60834368\n",
            "Iteration 950, loss = 0.60789198\n",
            "Iteration 951, loss = 0.60744062\n",
            "Iteration 952, loss = 0.60698791\n",
            "Iteration 953, loss = 0.60653547\n",
            "Iteration 954, loss = 0.60608165\n",
            "Iteration 955, loss = 0.60562848\n",
            "Iteration 956, loss = 0.60517370\n",
            "Iteration 957, loss = 0.60471927\n",
            "Iteration 958, loss = 0.60426402\n",
            "Iteration 959, loss = 0.60380805\n",
            "Iteration 960, loss = 0.60335239\n",
            "Iteration 961, loss = 0.60289520\n",
            "Iteration 962, loss = 0.60243833\n",
            "Iteration 963, loss = 0.60198065\n",
            "Iteration 964, loss = 0.60152227\n",
            "Iteration 965, loss = 0.60106398\n",
            "Iteration 966, loss = 0.60060460\n",
            "Iteration 967, loss = 0.60014480\n",
            "Iteration 968, loss = 0.59968520\n",
            "Iteration 969, loss = 0.59922441\n",
            "Iteration 970, loss = 0.59876297\n",
            "Iteration 971, loss = 0.59830189\n",
            "Iteration 972, loss = 0.59783920\n",
            "Iteration 973, loss = 0.59737680\n",
            "Iteration 974, loss = 0.59691376\n",
            "Iteration 975, loss = 0.59645001\n",
            "Iteration 976, loss = 0.59598562\n",
            "Iteration 977, loss = 0.59552159\n",
            "Iteration 978, loss = 0.59505591\n",
            "Iteration 979, loss = 0.59459012\n",
            "Iteration 980, loss = 0.59412449\n",
            "Iteration 981, loss = 0.59365777\n",
            "Iteration 982, loss = 0.59319040\n",
            "Iteration 983, loss = 0.59272243\n",
            "Iteration 984, loss = 0.59225466\n",
            "Iteration 985, loss = 0.59178554\n",
            "Iteration 986, loss = 0.59131616\n",
            "Iteration 987, loss = 0.59084674\n",
            "Iteration 988, loss = 0.59037638\n",
            "Iteration 989, loss = 0.58990554\n",
            "Iteration 990, loss = 0.58943409\n",
            "Iteration 991, loss = 0.58896213\n",
            "Iteration 992, loss = 0.58849015\n",
            "Iteration 993, loss = 0.58801724\n",
            "Iteration 994, loss = 0.58754374\n",
            "Iteration 995, loss = 0.58706967\n",
            "Iteration 996, loss = 0.58659528\n",
            "Iteration 997, loss = 0.58612044\n",
            "Iteration 998, loss = 0.58564493\n",
            "Iteration 999, loss = 0.58516885\n",
            "Iteration 1000, loss = 0.58469220\n",
            "Iteration 1001, loss = 0.58421515\n",
            "Iteration 1002, loss = 0.58373777\n",
            "Iteration 1003, loss = 0.58325973\n",
            "Iteration 1004, loss = 0.58278120\n",
            "Iteration 1005, loss = 0.58230211\n",
            "Iteration 1006, loss = 0.58182249\n",
            "Iteration 1007, loss = 0.58134233\n",
            "Iteration 1008, loss = 0.58086220\n",
            "Iteration 1009, loss = 0.58038080\n",
            "Iteration 1010, loss = 0.57989920\n",
            "Iteration 1011, loss = 0.57941704\n",
            "Iteration 1012, loss = 0.57893433\n",
            "Iteration 1013, loss = 0.57845108\n",
            "Iteration 1014, loss = 0.57796731\n",
            "Iteration 1015, loss = 0.57748301\n",
            "Iteration 1016, loss = 0.57699838\n",
            "Iteration 1017, loss = 0.57651306\n",
            "Iteration 1018, loss = 0.57602727\n",
            "Iteration 1019, loss = 0.57554092\n",
            "Iteration 1020, loss = 0.57505402\n",
            "Iteration 1021, loss = 0.57456659\n",
            "Iteration 1022, loss = 0.57407862\n",
            "Iteration 1023, loss = 0.57359012\n",
            "Iteration 1024, loss = 0.57310109\n",
            "Iteration 1025, loss = 0.57261155\n",
            "Iteration 1026, loss = 0.57212148\n",
            "Iteration 1027, loss = 0.57163089\n",
            "Iteration 1028, loss = 0.57113977\n",
            "Iteration 1029, loss = 0.57064856\n",
            "Iteration 1030, loss = 0.57015734\n",
            "Iteration 1031, loss = 0.56966490\n",
            "Iteration 1032, loss = 0.56917298\n",
            "Iteration 1033, loss = 0.56867995\n",
            "Iteration 1034, loss = 0.56818751\n",
            "Iteration 1035, loss = 0.56769408\n",
            "Iteration 1036, loss = 0.56720048\n",
            "Iteration 1037, loss = 0.56670643\n",
            "Iteration 1038, loss = 0.56621191\n",
            "Iteration 1039, loss = 0.56571693\n",
            "Iteration 1040, loss = 0.56522147\n",
            "Iteration 1041, loss = 0.56472554\n",
            "Iteration 1042, loss = 0.56422914\n",
            "Iteration 1043, loss = 0.56373226\n",
            "Iteration 1044, loss = 0.56323489\n",
            "Iteration 1045, loss = 0.56273703\n",
            "Iteration 1046, loss = 0.56223867\n",
            "Iteration 1047, loss = 0.56173982\n",
            "Iteration 1048, loss = 0.56124046\n",
            "Iteration 1049, loss = 0.56074060\n",
            "Iteration 1050, loss = 0.56024024\n",
            "Iteration 1051, loss = 0.55973936\n",
            "Iteration 1052, loss = 0.55923798\n",
            "Iteration 1053, loss = 0.55873611\n",
            "Iteration 1054, loss = 0.55823402\n",
            "Iteration 1055, loss = 0.55773129\n",
            "Iteration 1056, loss = 0.55722801\n",
            "Iteration 1057, loss = 0.55672420\n",
            "Iteration 1058, loss = 0.55621984\n",
            "Iteration 1059, loss = 0.55571495\n",
            "Iteration 1060, loss = 0.55520966\n",
            "Iteration 1061, loss = 0.55470413\n",
            "Iteration 1062, loss = 0.55419788\n",
            "Iteration 1063, loss = 0.55369107\n",
            "Iteration 1064, loss = 0.55318372\n",
            "Iteration 1065, loss = 0.55267587\n",
            "Iteration 1066, loss = 0.55216801\n",
            "Iteration 1067, loss = 0.55165931\n",
            "Iteration 1068, loss = 0.55115003\n",
            "Iteration 1069, loss = 0.55064021\n",
            "Iteration 1070, loss = 0.55013001\n",
            "Iteration 1071, loss = 0.54961960\n",
            "Iteration 1072, loss = 0.54910843\n",
            "Iteration 1073, loss = 0.54859668\n",
            "Iteration 1074, loss = 0.54808438\n",
            "Iteration 1075, loss = 0.54757260\n",
            "Iteration 1076, loss = 0.54705885\n",
            "Iteration 1077, loss = 0.54654521\n",
            "Iteration 1078, loss = 0.54603157\n",
            "Iteration 1079, loss = 0.54551702\n",
            "Iteration 1080, loss = 0.54500199\n",
            "Iteration 1081, loss = 0.54448634\n",
            "Iteration 1082, loss = 0.54397083\n",
            "Iteration 1083, loss = 0.54345420\n",
            "Iteration 1084, loss = 0.54293717\n",
            "Iteration 1085, loss = 0.54242026\n",
            "Iteration 1086, loss = 0.54190227\n",
            "Iteration 1087, loss = 0.54138384\n",
            "Iteration 1088, loss = 0.54086549\n",
            "Iteration 1089, loss = 0.54034615\n",
            "Iteration 1090, loss = 0.53982629\n",
            "Iteration 1091, loss = 0.53930672\n",
            "Iteration 1092, loss = 0.53878578\n",
            "Iteration 1093, loss = 0.53826472\n",
            "Iteration 1094, loss = 0.53774374\n",
            "Iteration 1095, loss = 0.53722162\n",
            "Iteration 1096, loss = 0.53669900\n",
            "Iteration 1097, loss = 0.53617660\n",
            "Iteration 1098, loss = 0.53565300\n",
            "Iteration 1099, loss = 0.53512984\n",
            "Iteration 1100, loss = 0.53460508\n",
            "Iteration 1101, loss = 0.53408113\n",
            "Iteration 1102, loss = 0.53355564\n",
            "Iteration 1103, loss = 0.53303043\n",
            "Iteration 1104, loss = 0.53250460\n",
            "Iteration 1105, loss = 0.53197922\n",
            "Iteration 1106, loss = 0.53145421\n",
            "Iteration 1107, loss = 0.53092747\n",
            "Iteration 1108, loss = 0.53040133\n",
            "Iteration 1109, loss = 0.52987454\n",
            "Iteration 1110, loss = 0.52934766\n",
            "Iteration 1111, loss = 0.52882087\n",
            "Iteration 1112, loss = 0.52829301\n",
            "Iteration 1113, loss = 0.52776562\n",
            "Iteration 1114, loss = 0.52723724\n",
            "Iteration 1115, loss = 0.52670843\n",
            "Iteration 1116, loss = 0.52618083\n",
            "Iteration 1117, loss = 0.52565077\n",
            "Iteration 1118, loss = 0.52512170\n",
            "Iteration 1119, loss = 0.52459234\n",
            "Iteration 1120, loss = 0.52406215\n",
            "Iteration 1121, loss = 0.52353146\n",
            "Iteration 1122, loss = 0.52300199\n",
            "Iteration 1123, loss = 0.52247020\n",
            "Iteration 1124, loss = 0.52193930\n",
            "Iteration 1125, loss = 0.52140823\n",
            "Iteration 1126, loss = 0.52087636\n",
            "Iteration 1127, loss = 0.52034402\n",
            "Iteration 1128, loss = 0.51981167\n",
            "Iteration 1129, loss = 0.51927931\n",
            "Iteration 1130, loss = 0.51874624\n",
            "Iteration 1131, loss = 0.51821279\n",
            "Iteration 1132, loss = 0.51767944\n",
            "Iteration 1133, loss = 0.51714579\n",
            "Iteration 1134, loss = 0.51661166\n",
            "Iteration 1135, loss = 0.51607716\n",
            "Iteration 1136, loss = 0.51554234\n",
            "Iteration 1137, loss = 0.51500740\n",
            "Iteration 1138, loss = 0.51447251\n",
            "Iteration 1139, loss = 0.51393705\n",
            "Iteration 1140, loss = 0.51340124\n",
            "Iteration 1141, loss = 0.51286513\n",
            "Iteration 1142, loss = 0.51232873\n",
            "Iteration 1143, loss = 0.51179209\n",
            "Iteration 1144, loss = 0.51125520\n",
            "Iteration 1145, loss = 0.51071911\n",
            "Iteration 1146, loss = 0.51018105\n",
            "Iteration 1147, loss = 0.50964355\n",
            "Iteration 1148, loss = 0.50910577\n",
            "Iteration 1149, loss = 0.50856771\n",
            "Iteration 1150, loss = 0.50802940\n",
            "Iteration 1151, loss = 0.50749084\n",
            "Iteration 1152, loss = 0.50695206\n",
            "Iteration 1153, loss = 0.50641307\n",
            "Iteration 1154, loss = 0.50587386\n",
            "Iteration 1155, loss = 0.50533444\n",
            "Iteration 1156, loss = 0.50479481\n",
            "Iteration 1157, loss = 0.50425497\n",
            "Iteration 1158, loss = 0.50371491\n",
            "Iteration 1159, loss = 0.50317465\n",
            "Iteration 1160, loss = 0.50263418\n",
            "Iteration 1161, loss = 0.50209351\n",
            "Iteration 1162, loss = 0.50155263\n",
            "Iteration 1163, loss = 0.50101156\n",
            "Iteration 1164, loss = 0.50047028\n",
            "Iteration 1165, loss = 0.49992881\n",
            "Iteration 1166, loss = 0.49938714\n",
            "Iteration 1167, loss = 0.49884526\n",
            "Iteration 1168, loss = 0.49830319\n",
            "Iteration 1169, loss = 0.49776093\n",
            "Iteration 1170, loss = 0.49721846\n",
            "Iteration 1171, loss = 0.49667580\n",
            "Iteration 1172, loss = 0.49613295\n",
            "Iteration 1173, loss = 0.49558990\n",
            "Iteration 1174, loss = 0.49504665\n",
            "Iteration 1175, loss = 0.49450322\n",
            "Iteration 1176, loss = 0.49395959\n",
            "Iteration 1177, loss = 0.49341577\n",
            "Iteration 1178, loss = 0.49287177\n",
            "Iteration 1179, loss = 0.49232757\n",
            "Iteration 1180, loss = 0.49178319\n",
            "Iteration 1181, loss = 0.49123862\n",
            "Iteration 1182, loss = 0.49069387\n",
            "Iteration 1183, loss = 0.49014894\n",
            "Iteration 1184, loss = 0.48960383\n",
            "Iteration 1185, loss = 0.48905853\n",
            "Iteration 1186, loss = 0.48851306\n",
            "Iteration 1187, loss = 0.48796742\n",
            "Iteration 1188, loss = 0.48742160\n",
            "Iteration 1189, loss = 0.48687561\n",
            "Iteration 1190, loss = 0.48632945\n",
            "Iteration 1191, loss = 0.48578312\n",
            "Iteration 1192, loss = 0.48523662\n",
            "Iteration 1193, loss = 0.48468996\n",
            "Iteration 1194, loss = 0.48414313\n",
            "Iteration 1195, loss = 0.48359615\n",
            "Iteration 1196, loss = 0.48304900\n",
            "Iteration 1197, loss = 0.48250170\n",
            "Iteration 1198, loss = 0.48195425\n",
            "Iteration 1199, loss = 0.48140664\n",
            "Iteration 1200, loss = 0.48085888\n",
            "Iteration 1201, loss = 0.48031098\n",
            "Iteration 1202, loss = 0.47976292\n",
            "Iteration 1203, loss = 0.47921473\n",
            "Iteration 1204, loss = 0.47866639\n",
            "Iteration 1205, loss = 0.47811792\n",
            "Iteration 1206, loss = 0.47756931\n",
            "Iteration 1207, loss = 0.47702056\n",
            "Iteration 1208, loss = 0.47647168\n",
            "Iteration 1209, loss = 0.47592268\n",
            "Iteration 1210, loss = 0.47537354\n",
            "Iteration 1211, loss = 0.47482429\n",
            "Iteration 1212, loss = 0.47427491\n",
            "Iteration 1213, loss = 0.47372541\n",
            "Iteration 1214, loss = 0.47317580\n",
            "Iteration 1215, loss = 0.47262608\n",
            "Iteration 1216, loss = 0.47207626\n",
            "Iteration 1217, loss = 0.47152639\n",
            "Iteration 1218, loss = 0.47097639\n",
            "Iteration 1219, loss = 0.47042627\n",
            "Iteration 1220, loss = 0.46987602\n",
            "Iteration 1221, loss = 0.46932565\n",
            "Iteration 1222, loss = 0.46877518\n",
            "Iteration 1223, loss = 0.46822462\n",
            "Iteration 1224, loss = 0.46767419\n",
            "Iteration 1225, loss = 0.46712338\n",
            "Iteration 1226, loss = 0.46657262\n",
            "Iteration 1227, loss = 0.46602175\n",
            "Iteration 1228, loss = 0.46547080\n",
            "Iteration 1229, loss = 0.46491985\n",
            "Iteration 1230, loss = 0.46436883\n",
            "Iteration 1231, loss = 0.46381771\n",
            "Iteration 1232, loss = 0.46326649\n",
            "Iteration 1233, loss = 0.46271519\n",
            "Iteration 1234, loss = 0.46216384\n",
            "Iteration 1235, loss = 0.46161268\n",
            "Iteration 1236, loss = 0.46106113\n",
            "Iteration 1237, loss = 0.46050967\n",
            "Iteration 1238, loss = 0.45995815\n",
            "Iteration 1239, loss = 0.45940658\n",
            "Iteration 1240, loss = 0.45885507\n",
            "Iteration 1241, loss = 0.45830347\n",
            "Iteration 1242, loss = 0.45775184\n",
            "Iteration 1243, loss = 0.45720015\n",
            "Iteration 1244, loss = 0.45664843\n",
            "Iteration 1245, loss = 0.45609669\n",
            "Iteration 1246, loss = 0.45554505\n",
            "Iteration 1247, loss = 0.45499331\n",
            "Iteration 1248, loss = 0.45444157\n",
            "Iteration 1249, loss = 0.45388981\n",
            "Iteration 1250, loss = 0.45333805\n",
            "Iteration 1251, loss = 0.45278846\n",
            "Iteration 1252, loss = 0.45223769\n",
            "Iteration 1253, loss = 0.45168658\n",
            "Iteration 1254, loss = 0.45113541\n",
            "Iteration 1255, loss = 0.45058502\n",
            "Iteration 1256, loss = 0.45003428\n",
            "Iteration 1257, loss = 0.44948439\n",
            "Iteration 1258, loss = 0.44893388\n",
            "Iteration 1259, loss = 0.44838357\n",
            "Iteration 1260, loss = 0.44783366\n",
            "Iteration 1261, loss = 0.44728348\n",
            "Iteration 1262, loss = 0.44673433\n",
            "Iteration 1263, loss = 0.44618420\n",
            "Iteration 1264, loss = 0.44563533\n",
            "Iteration 1265, loss = 0.44508586\n",
            "Iteration 1266, loss = 0.44453637\n",
            "Iteration 1267, loss = 0.44398780\n",
            "Iteration 1268, loss = 0.44343863\n",
            "Iteration 1269, loss = 0.44289051\n",
            "Iteration 1270, loss = 0.44234142\n",
            "Iteration 1271, loss = 0.44179361\n",
            "Iteration 1272, loss = 0.44124530\n",
            "Iteration 1273, loss = 0.44069706\n",
            "Iteration 1274, loss = 0.44014903\n",
            "Iteration 1275, loss = 0.43960165\n",
            "Iteration 1276, loss = 0.43905409\n",
            "Iteration 1277, loss = 0.43850668\n",
            "Iteration 1278, loss = 0.43795957\n",
            "Iteration 1279, loss = 0.43741283\n",
            "Iteration 1280, loss = 0.43686611\n",
            "Iteration 1281, loss = 0.43631956\n",
            "Iteration 1282, loss = 0.43577318\n",
            "Iteration 1283, loss = 0.43522703\n",
            "Iteration 1284, loss = 0.43468109\n",
            "Iteration 1285, loss = 0.43413541\n",
            "Iteration 1286, loss = 0.43359004\n",
            "Iteration 1287, loss = 0.43304482\n",
            "Iteration 1288, loss = 0.43249979\n",
            "Iteration 1289, loss = 0.43195498\n",
            "Iteration 1290, loss = 0.43141038\n",
            "Iteration 1291, loss = 0.43086602\n",
            "Iteration 1292, loss = 0.43032192\n",
            "Iteration 1293, loss = 0.42977805\n",
            "Iteration 1294, loss = 0.42923444\n",
            "Iteration 1295, loss = 0.42869109\n",
            "Iteration 1296, loss = 0.42814799\n",
            "Iteration 1297, loss = 0.42760514\n",
            "Iteration 1298, loss = 0.42706254\n",
            "Iteration 1299, loss = 0.42652021\n",
            "Iteration 1300, loss = 0.42597813\n",
            "Iteration 1301, loss = 0.42543632\n",
            "Iteration 1302, loss = 0.42489477\n",
            "Iteration 1303, loss = 0.42435349\n",
            "Iteration 1304, loss = 0.42381248\n",
            "Iteration 1305, loss = 0.42327174\n",
            "Iteration 1306, loss = 0.42273126\n",
            "Iteration 1307, loss = 0.42219106\n",
            "Iteration 1308, loss = 0.42165113\n",
            "Iteration 1309, loss = 0.42111148\n",
            "Iteration 1310, loss = 0.42057211\n",
            "Iteration 1311, loss = 0.42003302\n",
            "Iteration 1312, loss = 0.41949420\n",
            "Iteration 1313, loss = 0.41895568\n",
            "Iteration 1314, loss = 0.41841744\n",
            "Iteration 1315, loss = 0.41787948\n",
            "Iteration 1316, loss = 0.41734182\n",
            "Iteration 1317, loss = 0.41680446\n",
            "Iteration 1318, loss = 0.41626738\n",
            "Iteration 1319, loss = 0.41573061\n",
            "Iteration 1320, loss = 0.41519414\n",
            "Iteration 1321, loss = 0.41465797\n",
            "Iteration 1322, loss = 0.41412211\n",
            "Iteration 1323, loss = 0.41358656\n",
            "Iteration 1324, loss = 0.41305132\n",
            "Iteration 1325, loss = 0.41251639\n",
            "Iteration 1326, loss = 0.41198178\n",
            "Iteration 1327, loss = 0.41144749\n",
            "Iteration 1328, loss = 0.41091352\n",
            "Iteration 1329, loss = 0.41037988\n",
            "Iteration 1330, loss = 0.40984657\n",
            "Iteration 1331, loss = 0.40931359\n",
            "Iteration 1332, loss = 0.40878094\n",
            "Iteration 1333, loss = 0.40824863\n",
            "Iteration 1334, loss = 0.40771666\n",
            "Iteration 1335, loss = 0.40718502\n",
            "Iteration 1336, loss = 0.40665374\n",
            "Iteration 1337, loss = 0.40612280\n",
            "Iteration 1338, loss = 0.40559222\n",
            "Iteration 1339, loss = 0.40506198\n",
            "Iteration 1340, loss = 0.40453211\n",
            "Iteration 1341, loss = 0.40400260\n",
            "Iteration 1342, loss = 0.40347344\n",
            "Iteration 1343, loss = 0.40294465\n",
            "Iteration 1344, loss = 0.40241624\n",
            "Iteration 1345, loss = 0.40188819\n",
            "Iteration 1346, loss = 0.40136052\n",
            "Iteration 1347, loss = 0.40083323\n",
            "Iteration 1348, loss = 0.40030633\n",
            "Iteration 1349, loss = 0.39977980\n",
            "Iteration 1350, loss = 0.39925366\n",
            "Iteration 1351, loss = 0.39872792\n",
            "Iteration 1352, loss = 0.39820257\n",
            "Iteration 1353, loss = 0.39767761\n",
            "Iteration 1354, loss = 0.39715305\n",
            "Iteration 1355, loss = 0.39662891\n",
            "Iteration 1356, loss = 0.39610516\n",
            "Iteration 1357, loss = 0.39558183\n",
            "Iteration 1358, loss = 0.39505891\n",
            "Iteration 1359, loss = 0.39453640\n",
            "Iteration 1360, loss = 0.39401432\n",
            "Iteration 1361, loss = 0.39349266\n",
            "Iteration 1362, loss = 0.39297142\n",
            "Iteration 1363, loss = 0.39245062\n",
            "Iteration 1364, loss = 0.39193024\n",
            "Iteration 1365, loss = 0.39141031\n",
            "Iteration 1366, loss = 0.39089081\n",
            "Iteration 1367, loss = 0.39037176\n",
            "Iteration 1368, loss = 0.38985315\n",
            "Iteration 1369, loss = 0.38933499\n",
            "Iteration 1370, loss = 0.38881728\n",
            "Iteration 1371, loss = 0.38830003\n",
            "Iteration 1372, loss = 0.38778323\n",
            "Iteration 1373, loss = 0.38726690\n",
            "Iteration 1374, loss = 0.38675104\n",
            "Iteration 1375, loss = 0.38623564\n",
            "Iteration 1376, loss = 0.38572072\n",
            "Iteration 1377, loss = 0.38520626\n",
            "Iteration 1378, loss = 0.38469230\n",
            "Iteration 1379, loss = 0.38417880\n",
            "Iteration 1380, loss = 0.38366580\n",
            "Iteration 1381, loss = 0.38315328\n",
            "Iteration 1382, loss = 0.38264126\n",
            "Iteration 1383, loss = 0.38212973\n",
            "Iteration 1384, loss = 0.38161869\n",
            "Iteration 1385, loss = 0.38110816\n",
            "Iteration 1386, loss = 0.38059813\n",
            "Iteration 1387, loss = 0.38008861\n",
            "Iteration 1388, loss = 0.37957960\n",
            "Iteration 1389, loss = 0.37907111\n",
            "Iteration 1390, loss = 0.37856313\n",
            "Iteration 1391, loss = 0.37805567\n",
            "Iteration 1392, loss = 0.37754874\n",
            "Iteration 1393, loss = 0.37704233\n",
            "Iteration 1394, loss = 0.37653645\n",
            "Iteration 1395, loss = 0.37603110\n",
            "Iteration 1396, loss = 0.37552629\n",
            "Iteration 1397, loss = 0.37502202\n",
            "Iteration 1398, loss = 0.37451830\n",
            "Iteration 1399, loss = 0.37401511\n",
            "Iteration 1400, loss = 0.37351247\n",
            "Iteration 1401, loss = 0.37301039\n",
            "Iteration 1402, loss = 0.37250886\n",
            "Iteration 1403, loss = 0.37200789\n",
            "Iteration 1404, loss = 0.37150747\n",
            "Iteration 1405, loss = 0.37100762\n",
            "Iteration 1406, loss = 0.37050834\n",
            "Iteration 1407, loss = 0.37000963\n",
            "Iteration 1408, loss = 0.36951149\n",
            "Iteration 1409, loss = 0.36901392\n",
            "Iteration 1410, loss = 0.36851693\n",
            "Iteration 1411, loss = 0.36802053\n",
            "Iteration 1412, loss = 0.36752470\n",
            "Iteration 1413, loss = 0.36702947\n",
            "Iteration 1414, loss = 0.36653482\n",
            "Iteration 1415, loss = 0.36604077\n",
            "Iteration 1416, loss = 0.36554731\n",
            "Iteration 1417, loss = 0.36505445\n",
            "Iteration 1418, loss = 0.36456219\n",
            "Iteration 1419, loss = 0.36407054\n",
            "Iteration 1420, loss = 0.36357949\n",
            "Iteration 1421, loss = 0.36308905\n",
            "Iteration 1422, loss = 0.36259922\n",
            "Iteration 1423, loss = 0.36211001\n",
            "Iteration 1424, loss = 0.36162141\n",
            "Iteration 1425, loss = 0.36113344\n",
            "Iteration 1426, loss = 0.36064609\n",
            "Iteration 1427, loss = 0.36015936\n",
            "Iteration 1428, loss = 0.35967326\n",
            "Iteration 1429, loss = 0.35918780\n",
            "Iteration 1430, loss = 0.35870296\n",
            "Iteration 1431, loss = 0.35821876\n",
            "Iteration 1432, loss = 0.35773520\n",
            "Iteration 1433, loss = 0.35725228\n",
            "Iteration 1434, loss = 0.35677001\n",
            "Iteration 1435, loss = 0.35628838\n",
            "Iteration 1436, loss = 0.35580739\n",
            "Iteration 1437, loss = 0.35532706\n",
            "Iteration 1438, loss = 0.35484738\n",
            "Iteration 1439, loss = 0.35436961\n",
            "Iteration 1440, loss = 0.35389231\n",
            "Iteration 1441, loss = 0.35341555\n",
            "Iteration 1442, loss = 0.35293958\n",
            "Iteration 1443, loss = 0.35246442\n",
            "Iteration 1444, loss = 0.35199062\n",
            "Iteration 1445, loss = 0.35151656\n",
            "Iteration 1446, loss = 0.35104353\n",
            "Iteration 1447, loss = 0.35057116\n",
            "Iteration 1448, loss = 0.35009990\n",
            "Iteration 1449, loss = 0.34962902\n",
            "Iteration 1450, loss = 0.34915903\n",
            "Iteration 1451, loss = 0.34868974\n",
            "Iteration 1452, loss = 0.34822144\n",
            "Iteration 1453, loss = 0.34775388\n",
            "Iteration 1454, loss = 0.34728690\n",
            "Iteration 1455, loss = 0.34682087\n",
            "Iteration 1456, loss = 0.34635558\n",
            "Iteration 1457, loss = 0.34589085\n",
            "Iteration 1458, loss = 0.34542684\n",
            "Iteration 1459, loss = 0.34496422\n",
            "Iteration 1460, loss = 0.34450148\n",
            "Iteration 1461, loss = 0.34403984\n",
            "Iteration 1462, loss = 0.34357895\n",
            "Iteration 1463, loss = 0.34311882\n",
            "Iteration 1464, loss = 0.34265947\n",
            "Iteration 1465, loss = 0.34220097\n",
            "Iteration 1466, loss = 0.34174308\n",
            "Iteration 1467, loss = 0.34128599\n",
            "Iteration 1468, loss = 0.34082962\n",
            "Iteration 1469, loss = 0.34037399\n",
            "Iteration 1470, loss = 0.33991909\n",
            "Iteration 1471, loss = 0.33946495\n",
            "Iteration 1472, loss = 0.33901155\n",
            "Iteration 1473, loss = 0.33855891\n",
            "Iteration 1474, loss = 0.33810701\n",
            "Iteration 1475, loss = 0.33765587\n",
            "Iteration 1476, loss = 0.33720547\n",
            "Iteration 1477, loss = 0.33675581\n",
            "Iteration 1478, loss = 0.33630690\n",
            "Iteration 1479, loss = 0.33585874\n",
            "Iteration 1480, loss = 0.33541132\n",
            "Iteration 1481, loss = 0.33496465\n",
            "Iteration 1482, loss = 0.33451873\n",
            "Iteration 1483, loss = 0.33407355\n",
            "Iteration 1484, loss = 0.33362912\n",
            "Iteration 1485, loss = 0.33318544\n",
            "Iteration 1486, loss = 0.33274250\n",
            "Iteration 1487, loss = 0.33230031\n",
            "Iteration 1488, loss = 0.33185886\n",
            "Iteration 1489, loss = 0.33141816\n",
            "Iteration 1490, loss = 0.33097821\n",
            "Iteration 1491, loss = 0.33053900\n",
            "Iteration 1492, loss = 0.33010054\n",
            "Iteration 1493, loss = 0.32966283\n",
            "Iteration 1494, loss = 0.32922587\n",
            "Iteration 1495, loss = 0.32878966\n",
            "Iteration 1496, loss = 0.32835419\n",
            "Iteration 1497, loss = 0.32791948\n",
            "Iteration 1498, loss = 0.32748551\n",
            "Iteration 1499, loss = 0.32705230\n",
            "Iteration 1500, loss = 0.32661984\n",
            "Iteration 1501, loss = 0.32618813\n",
            "Iteration 1502, loss = 0.32575717\n",
            "Iteration 1503, loss = 0.32532697\n",
            "Iteration 1504, loss = 0.32489751\n",
            "Iteration 1505, loss = 0.32446881\n",
            "Iteration 1506, loss = 0.32404086\n",
            "Iteration 1507, loss = 0.32361367\n",
            "Iteration 1508, loss = 0.32318724\n",
            "Iteration 1509, loss = 0.32276155\n",
            "Iteration 1510, loss = 0.32233663\n",
            "Iteration 1511, loss = 0.32191245\n",
            "Iteration 1512, loss = 0.32148904\n",
            "Iteration 1513, loss = 0.32106638\n",
            "Iteration 1514, loss = 0.32064448\n",
            "Iteration 1515, loss = 0.32022333\n",
            "Iteration 1516, loss = 0.31980294\n",
            "Iteration 1517, loss = 0.31938331\n",
            "Iteration 1518, loss = 0.31896444\n",
            "Iteration 1519, loss = 0.31854632\n",
            "Iteration 1520, loss = 0.31812897\n",
            "Iteration 1521, loss = 0.31771237\n",
            "Iteration 1522, loss = 0.31729653\n",
            "Iteration 1523, loss = 0.31688146\n",
            "Iteration 1524, loss = 0.31646714\n",
            "Iteration 1525, loss = 0.31605358\n",
            "Iteration 1526, loss = 0.31564079\n",
            "Iteration 1527, loss = 0.31522875\n",
            "Iteration 1528, loss = 0.31481748\n",
            "Iteration 1529, loss = 0.31440696\n",
            "Iteration 1530, loss = 0.31399721\n",
            "Iteration 1531, loss = 0.31358822\n",
            "Iteration 1532, loss = 0.31318000\n",
            "Iteration 1533, loss = 0.31277253\n",
            "Iteration 1534, loss = 0.31236583\n",
            "Iteration 1535, loss = 0.31195988\n",
            "Iteration 1536, loss = 0.31155471\n",
            "Iteration 1537, loss = 0.31115049\n",
            "Iteration 1538, loss = 0.31074712\n",
            "Iteration 1539, loss = 0.31034455\n",
            "Iteration 1540, loss = 0.30994281\n",
            "Iteration 1541, loss = 0.30954191\n",
            "Iteration 1542, loss = 0.30914185\n",
            "Iteration 1543, loss = 0.30874259\n",
            "Iteration 1544, loss = 0.30834412\n",
            "Iteration 1545, loss = 0.30794642\n",
            "Iteration 1546, loss = 0.30754948\n",
            "Iteration 1547, loss = 0.30715333\n",
            "Iteration 1548, loss = 0.30675794\n",
            "Iteration 1549, loss = 0.30636332\n",
            "Iteration 1550, loss = 0.30596947\n",
            "Iteration 1551, loss = 0.30557640\n",
            "Iteration 1552, loss = 0.30518409\n",
            "Iteration 1553, loss = 0.30479256\n",
            "Iteration 1554, loss = 0.30440181\n",
            "Iteration 1555, loss = 0.30401182\n",
            "Iteration 1556, loss = 0.30362261\n",
            "Iteration 1557, loss = 0.30323416\n",
            "Iteration 1558, loss = 0.30284648\n",
            "Iteration 1559, loss = 0.30245956\n",
            "Iteration 1560, loss = 0.30207341\n",
            "Iteration 1561, loss = 0.30168803\n",
            "Iteration 1562, loss = 0.30130342\n",
            "Iteration 1563, loss = 0.30091957\n",
            "Iteration 1564, loss = 0.30053650\n",
            "Iteration 1565, loss = 0.30015418\n",
            "Iteration 1566, loss = 0.29977264\n",
            "Iteration 1567, loss = 0.29939185\n",
            "Iteration 1568, loss = 0.29901183\n",
            "Iteration 1569, loss = 0.29863258\n",
            "Iteration 1570, loss = 0.29825409\n",
            "Iteration 1571, loss = 0.29787636\n",
            "Iteration 1572, loss = 0.29749948\n",
            "Iteration 1573, loss = 0.29712362\n",
            "Iteration 1574, loss = 0.29674869\n",
            "Iteration 1575, loss = 0.29637473\n",
            "Iteration 1576, loss = 0.29600133\n",
            "Iteration 1577, loss = 0.29562884\n",
            "Iteration 1578, loss = 0.29525715\n",
            "Iteration 1579, loss = 0.29488611\n",
            "Iteration 1580, loss = 0.29451581\n",
            "Iteration 1581, loss = 0.29414648\n",
            "Iteration 1582, loss = 0.29377780\n",
            "Iteration 1583, loss = 0.29340991\n",
            "Iteration 1584, loss = 0.29304279\n",
            "Iteration 1585, loss = 0.29267679\n",
            "Iteration 1586, loss = 0.29231129\n",
            "Iteration 1587, loss = 0.29194654\n",
            "Iteration 1588, loss = 0.29158306\n",
            "Iteration 1589, loss = 0.29121970\n",
            "Iteration 1590, loss = 0.29085762\n",
            "Iteration 1591, loss = 0.29049614\n",
            "Iteration 1592, loss = 0.29013553\n",
            "Iteration 1593, loss = 0.28977603\n",
            "Iteration 1594, loss = 0.28941748\n",
            "Iteration 1595, loss = 0.28905954\n",
            "Iteration 1596, loss = 0.28870292\n",
            "Iteration 1597, loss = 0.28834669\n",
            "Iteration 1598, loss = 0.28799199\n",
            "Iteration 1599, loss = 0.28763750\n",
            "Iteration 1600, loss = 0.28728410\n",
            "Iteration 1601, loss = 0.28693155\n",
            "Iteration 1602, loss = 0.28658000\n",
            "Iteration 1603, loss = 0.28622912\n",
            "Iteration 1604, loss = 0.28587906\n",
            "Iteration 1605, loss = 0.28552976\n",
            "Iteration 1606, loss = 0.28518124\n",
            "Iteration 1607, loss = 0.28483350\n",
            "Iteration 1608, loss = 0.28448654\n",
            "Iteration 1609, loss = 0.28414035\n",
            "Iteration 1610, loss = 0.28379493\n",
            "Iteration 1611, loss = 0.28345026\n",
            "Iteration 1612, loss = 0.28310635\n",
            "Iteration 1613, loss = 0.28276320\n",
            "Iteration 1614, loss = 0.28242080\n",
            "Iteration 1615, loss = 0.28207915\n",
            "Iteration 1616, loss = 0.28173825\n",
            "Iteration 1617, loss = 0.28139810\n",
            "Iteration 1618, loss = 0.28105871\n",
            "Iteration 1619, loss = 0.28072006\n",
            "Iteration 1620, loss = 0.28038215\n",
            "Iteration 1621, loss = 0.28004499\n",
            "Iteration 1622, loss = 0.27970857\n",
            "Iteration 1623, loss = 0.27937290\n",
            "Iteration 1624, loss = 0.27903796\n",
            "Iteration 1625, loss = 0.27870377\n",
            "Iteration 1626, loss = 0.27837031\n",
            "Iteration 1627, loss = 0.27803758\n",
            "Iteration 1628, loss = 0.27770560\n",
            "Iteration 1629, loss = 0.27737434\n",
            "Iteration 1630, loss = 0.27704382\n",
            "Iteration 1631, loss = 0.27671403\n",
            "Iteration 1632, loss = 0.27638496\n",
            "Iteration 1633, loss = 0.27605663\n",
            "Iteration 1634, loss = 0.27572902\n",
            "Iteration 1635, loss = 0.27540214\n",
            "Iteration 1636, loss = 0.27507598\n",
            "Iteration 1637, loss = 0.27475055\n",
            "Iteration 1638, loss = 0.27442583\n",
            "Iteration 1639, loss = 0.27410183\n",
            "Iteration 1640, loss = 0.27377856\n",
            "Iteration 1641, loss = 0.27345600\n",
            "Iteration 1642, loss = 0.27313415\n",
            "Iteration 1643, loss = 0.27281302\n",
            "Iteration 1644, loss = 0.27249260\n",
            "Iteration 1645, loss = 0.27217289\n",
            "Iteration 1646, loss = 0.27185389\n",
            "Iteration 1647, loss = 0.27153559\n",
            "Iteration 1648, loss = 0.27121801\n",
            "Iteration 1649, loss = 0.27090113\n",
            "Iteration 1650, loss = 0.27058495\n",
            "Iteration 1651, loss = 0.27026947\n",
            "Iteration 1652, loss = 0.26995470\n",
            "Iteration 1653, loss = 0.26964062\n",
            "Iteration 1654, loss = 0.26932724\n",
            "Iteration 1655, loss = 0.26901456\n",
            "Iteration 1656, loss = 0.26870257\n",
            "Iteration 1657, loss = 0.26839128\n",
            "Iteration 1658, loss = 0.26808068\n",
            "Iteration 1659, loss = 0.26777077\n",
            "Iteration 1660, loss = 0.26746155\n",
            "Iteration 1661, loss = 0.26715301\n",
            "Iteration 1662, loss = 0.26684517\n",
            "Iteration 1663, loss = 0.26653800\n",
            "Iteration 1664, loss = 0.26623153\n",
            "Iteration 1665, loss = 0.26592573\n",
            "Iteration 1666, loss = 0.26562061\n",
            "Iteration 1667, loss = 0.26531618\n",
            "Iteration 1668, loss = 0.26501242\n",
            "Iteration 1669, loss = 0.26470934\n",
            "Iteration 1670, loss = 0.26440694\n",
            "Iteration 1671, loss = 0.26410520\n",
            "Iteration 1672, loss = 0.26380415\n",
            "Iteration 1673, loss = 0.26350376\n",
            "Iteration 1674, loss = 0.26320404\n",
            "Iteration 1675, loss = 0.26290499\n",
            "Iteration 1676, loss = 0.26260661\n",
            "Iteration 1677, loss = 0.26230889\n",
            "Iteration 1678, loss = 0.26201184\n",
            "Iteration 1679, loss = 0.26171545\n",
            "Iteration 1680, loss = 0.26141972\n",
            "Iteration 1681, loss = 0.26112465\n",
            "Iteration 1682, loss = 0.26083024\n",
            "Iteration 1683, loss = 0.26053649\n",
            "Iteration 1684, loss = 0.26024339\n",
            "Iteration 1685, loss = 0.25995095\n",
            "Iteration 1686, loss = 0.25965916\n",
            "Iteration 1687, loss = 0.25936802\n",
            "Iteration 1688, loss = 0.25907753\n",
            "Iteration 1689, loss = 0.25878769\n",
            "Iteration 1690, loss = 0.25849850\n",
            "Iteration 1691, loss = 0.25820995\n",
            "Iteration 1692, loss = 0.25792205\n",
            "Iteration 1693, loss = 0.25763479\n",
            "Iteration 1694, loss = 0.25734818\n",
            "Iteration 1695, loss = 0.25706220\n",
            "Iteration 1696, loss = 0.25677687\n",
            "Iteration 1697, loss = 0.25649217\n",
            "Iteration 1698, loss = 0.25620811\n",
            "Iteration 1699, loss = 0.25592468\n",
            "Iteration 1700, loss = 0.25564189\n",
            "Iteration 1701, loss = 0.25535972\n",
            "Iteration 1702, loss = 0.25507819\n",
            "Iteration 1703, loss = 0.25479729\n",
            "Iteration 1704, loss = 0.25451702\n",
            "Iteration 1705, loss = 0.25423737\n",
            "Iteration 1706, loss = 0.25395835\n",
            "Iteration 1707, loss = 0.25367996\n",
            "Iteration 1708, loss = 0.25340218\n",
            "Iteration 1709, loss = 0.25312502\n",
            "Iteration 1710, loss = 0.25284849\n",
            "Iteration 1711, loss = 0.25257258\n",
            "Iteration 1712, loss = 0.25229728\n",
            "Iteration 1713, loss = 0.25202259\n",
            "Iteration 1714, loss = 0.25174852\n",
            "Iteration 1715, loss = 0.25147507\n",
            "Iteration 1716, loss = 0.25120222\n",
            "Iteration 1717, loss = 0.25092999\n",
            "Iteration 1718, loss = 0.25065836\n",
            "Iteration 1719, loss = 0.25038734\n",
            "Iteration 1720, loss = 0.25011693\n",
            "Iteration 1721, loss = 0.24984712\n",
            "Iteration 1722, loss = 0.24957791\n",
            "Iteration 1723, loss = 0.24930931\n",
            "Iteration 1724, loss = 0.24904131\n",
            "Iteration 1725, loss = 0.24877390\n",
            "Iteration 1726, loss = 0.24850710\n",
            "Iteration 1727, loss = 0.24824089\n",
            "Iteration 1728, loss = 0.24797527\n",
            "Iteration 1729, loss = 0.24771025\n",
            "Iteration 1730, loss = 0.24744582\n",
            "Iteration 1731, loss = 0.24718198\n",
            "Iteration 1732, loss = 0.24691873\n",
            "Iteration 1733, loss = 0.24665607\n",
            "Iteration 1734, loss = 0.24639399\n",
            "Iteration 1735, loss = 0.24613250\n",
            "Iteration 1736, loss = 0.24587159\n",
            "Iteration 1737, loss = 0.24561127\n",
            "Iteration 1738, loss = 0.24535153\n",
            "Iteration 1739, loss = 0.24509237\n",
            "Iteration 1740, loss = 0.24483378\n",
            "Iteration 1741, loss = 0.24457578\n",
            "Iteration 1742, loss = 0.24431834\n",
            "Iteration 1743, loss = 0.24406149\n",
            "Iteration 1744, loss = 0.24380520\n",
            "Iteration 1745, loss = 0.24354949\n",
            "Iteration 1746, loss = 0.24329435\n",
            "Iteration 1747, loss = 0.24303978\n",
            "Iteration 1748, loss = 0.24278577\n",
            "Iteration 1749, loss = 0.24253233\n",
            "Iteration 1750, loss = 0.24227946\n",
            "Iteration 1751, loss = 0.24202715\n",
            "Iteration 1752, loss = 0.24177540\n",
            "Iteration 1753, loss = 0.24152421\n",
            "Iteration 1754, loss = 0.24127358\n",
            "Iteration 1755, loss = 0.24102351\n",
            "Iteration 1756, loss = 0.24077400\n",
            "Iteration 1757, loss = 0.24052504\n",
            "Iteration 1758, loss = 0.24027663\n",
            "Iteration 1759, loss = 0.24002878\n",
            "Iteration 1760, loss = 0.23978148\n",
            "Iteration 1761, loss = 0.23953473\n",
            "Iteration 1762, loss = 0.23928853\n",
            "Iteration 1763, loss = 0.23904288\n",
            "Iteration 1764, loss = 0.23879777\n",
            "Iteration 1765, loss = 0.23855321\n",
            "Iteration 1766, loss = 0.23830919\n",
            "Iteration 1767, loss = 0.23806571\n",
            "Iteration 1768, loss = 0.23782277\n",
            "Iteration 1769, loss = 0.23758037\n",
            "Iteration 1770, loss = 0.23733851\n",
            "Iteration 1771, loss = 0.23709719\n",
            "Iteration 1772, loss = 0.23685640\n",
            "Iteration 1773, loss = 0.23661615\n",
            "Iteration 1774, loss = 0.23637643\n",
            "Iteration 1775, loss = 0.23613724\n",
            "Iteration 1776, loss = 0.23589858\n",
            "Iteration 1777, loss = 0.23566045\n",
            "Iteration 1778, loss = 0.23542285\n",
            "Iteration 1779, loss = 0.23518577\n",
            "Iteration 1780, loss = 0.23494922\n",
            "Iteration 1781, loss = 0.23471319\n",
            "Iteration 1782, loss = 0.23447769\n",
            "Iteration 1783, loss = 0.23424270\n",
            "Iteration 1784, loss = 0.23400823\n",
            "Iteration 1785, loss = 0.23377429\n",
            "Iteration 1786, loss = 0.23354086\n",
            "Iteration 1787, loss = 0.23330794\n",
            "Iteration 1788, loss = 0.23307554\n",
            "Iteration 1789, loss = 0.23284366\n",
            "Iteration 1790, loss = 0.23261228\n",
            "Iteration 1791, loss = 0.23238142\n",
            "Iteration 1792, loss = 0.23215106\n",
            "Iteration 1793, loss = 0.23192122\n",
            "Iteration 1794, loss = 0.23169188\n",
            "Iteration 1795, loss = 0.23146304\n",
            "Iteration 1796, loss = 0.23123471\n",
            "Iteration 1797, loss = 0.23100689\n",
            "Iteration 1798, loss = 0.23077956\n",
            "Iteration 1799, loss = 0.23055273\n",
            "Iteration 1800, loss = 0.23032641\n",
            "Iteration 1801, loss = 0.23010058\n",
            "Iteration 1802, loss = 0.22987525\n",
            "Iteration 1803, loss = 0.22965041\n",
            "Iteration 1804, loss = 0.22942607\n",
            "Iteration 1805, loss = 0.22920222\n",
            "Iteration 1806, loss = 0.22897886\n",
            "Iteration 1807, loss = 0.22875599\n",
            "Iteration 1808, loss = 0.22853361\n",
            "Iteration 1809, loss = 0.22831172\n",
            "Iteration 1810, loss = 0.22809031\n",
            "Iteration 1811, loss = 0.22786939\n",
            "Iteration 1812, loss = 0.22764896\n",
            "Iteration 1813, loss = 0.22742900\n",
            "Iteration 1814, loss = 0.22720953\n",
            "Iteration 1815, loss = 0.22699054\n",
            "Iteration 1816, loss = 0.22677203\n",
            "Iteration 1817, loss = 0.22655399\n",
            "Iteration 1818, loss = 0.22633643\n",
            "Iteration 1819, loss = 0.22611935\n",
            "Iteration 1820, loss = 0.22590274\n",
            "Iteration 1821, loss = 0.22568660\n",
            "Iteration 1822, loss = 0.22547094\n",
            "Iteration 1823, loss = 0.22525574\n",
            "Iteration 1824, loss = 0.22504102\n",
            "Iteration 1825, loss = 0.22482676\n",
            "Iteration 1826, loss = 0.22461297\n",
            "Iteration 1827, loss = 0.22439965\n",
            "Iteration 1828, loss = 0.22418679\n",
            "Iteration 1829, loss = 0.22397439\n",
            "Iteration 1830, loss = 0.22376245\n",
            "Iteration 1831, loss = 0.22355098\n",
            "Iteration 1832, loss = 0.22333996\n",
            "Iteration 1833, loss = 0.22312940\n",
            "Iteration 1834, loss = 0.22291930\n",
            "Iteration 1835, loss = 0.22270966\n",
            "Iteration 1836, loss = 0.22250047\n",
            "Iteration 1837, loss = 0.22229173\n",
            "Iteration 1838, loss = 0.22208345\n",
            "Iteration 1839, loss = 0.22187562\n",
            "Iteration 1840, loss = 0.22166823\n",
            "Iteration 1841, loss = 0.22146130\n",
            "Iteration 1842, loss = 0.22125481\n",
            "Iteration 1843, loss = 0.22104877\n",
            "Iteration 1844, loss = 0.22084318\n",
            "Iteration 1845, loss = 0.22063803\n",
            "Iteration 1846, loss = 0.22043332\n",
            "Iteration 1847, loss = 0.22022905\n",
            "Iteration 1848, loss = 0.22002523\n",
            "Iteration 1849, loss = 0.21982184\n",
            "Iteration 1850, loss = 0.21961889\n",
            "Iteration 1851, loss = 0.21941638\n",
            "Iteration 1852, loss = 0.21921431\n",
            "Iteration 1853, loss = 0.21901266\n",
            "Iteration 1854, loss = 0.21881146\n",
            "Iteration 1855, loss = 0.21861068\n",
            "Iteration 1856, loss = 0.21841034\n",
            "Iteration 1857, loss = 0.21821043\n",
            "Iteration 1858, loss = 0.21801094\n",
            "Iteration 1859, loss = 0.21781189\n",
            "Iteration 1860, loss = 0.21761326\n",
            "Iteration 1861, loss = 0.21741506\n",
            "Iteration 1862, loss = 0.21721728\n",
            "Iteration 1863, loss = 0.21701992\n",
            "Iteration 1864, loss = 0.21682299\n",
            "Iteration 1865, loss = 0.21662648\n",
            "Iteration 1866, loss = 0.21643039\n",
            "Iteration 1867, loss = 0.21623471\n",
            "Iteration 1868, loss = 0.21603946\n",
            "Iteration 1869, loss = 0.21584462\n",
            "Iteration 1870, loss = 0.21565020\n",
            "Iteration 1871, loss = 0.21545619\n",
            "Iteration 1872, loss = 0.21526259\n",
            "Iteration 1873, loss = 0.21506941\n",
            "Iteration 1874, loss = 0.21487664\n",
            "Iteration 1875, loss = 0.21468428\n",
            "Iteration 1876, loss = 0.21449232\n",
            "Iteration 1877, loss = 0.21430078\n",
            "Iteration 1878, loss = 0.21410964\n",
            "Iteration 1879, loss = 0.21391891\n",
            "Iteration 1880, loss = 0.21372858\n",
            "Iteration 1881, loss = 0.21353866\n",
            "Iteration 1882, loss = 0.21334914\n",
            "Iteration 1883, loss = 0.21316002\n",
            "Iteration 1884, loss = 0.21297130\n",
            "Iteration 1885, loss = 0.21278298\n",
            "Iteration 1886, loss = 0.21259505\n",
            "Iteration 1887, loss = 0.21240753\n",
            "Iteration 1888, loss = 0.21222040\n",
            "Iteration 1889, loss = 0.21203366\n",
            "Iteration 1890, loss = 0.21184732\n",
            "Iteration 1891, loss = 0.21166138\n",
            "Iteration 1892, loss = 0.21147582\n",
            "Iteration 1893, loss = 0.21129066\n",
            "Iteration 1894, loss = 0.21110588\n",
            "Iteration 1895, loss = 0.21092149\n",
            "Iteration 1896, loss = 0.21073750\n",
            "Iteration 1897, loss = 0.21055388\n",
            "Iteration 1898, loss = 0.21037066\n",
            "Iteration 1899, loss = 0.21018781\n",
            "Iteration 1900, loss = 0.21000536\n",
            "Iteration 1901, loss = 0.20982328\n",
            "Iteration 1902, loss = 0.20964158\n",
            "Iteration 1903, loss = 0.20946027\n",
            "Iteration 1904, loss = 0.20927933\n",
            "Iteration 1905, loss = 0.20909878\n",
            "Iteration 1906, loss = 0.20891860\n",
            "Iteration 1907, loss = 0.20873880\n",
            "Iteration 1908, loss = 0.20855937\n",
            "Iteration 1909, loss = 0.20838031\n",
            "Iteration 1910, loss = 0.20820195\n",
            "Iteration 1911, loss = 0.20802393\n",
            "Iteration 1912, loss = 0.20784631\n",
            "Iteration 1913, loss = 0.20766912\n",
            "Iteration 1914, loss = 0.20749235\n",
            "Iteration 1915, loss = 0.20731601\n",
            "Iteration 1916, loss = 0.20714007\n",
            "Iteration 1917, loss = 0.20696452\n",
            "Iteration 1918, loss = 0.20678933\n",
            "Iteration 1919, loss = 0.20661452\n",
            "Iteration 1920, loss = 0.20644007\n",
            "Iteration 1921, loss = 0.20626598\n",
            "Iteration 1922, loss = 0.20609225\n",
            "Iteration 1923, loss = 0.20591889\n",
            "Iteration 1924, loss = 0.20574589\n",
            "Iteration 1925, loss = 0.20557327\n",
            "Iteration 1926, loss = 0.20540100\n",
            "Iteration 1927, loss = 0.20522910\n",
            "Iteration 1928, loss = 0.20505756\n",
            "Iteration 1929, loss = 0.20488637\n",
            "Iteration 1930, loss = 0.20471553\n",
            "Iteration 1931, loss = 0.20454505\n",
            "Iteration 1932, loss = 0.20437492\n",
            "Iteration 1933, loss = 0.20420514\n",
            "Iteration 1934, loss = 0.20403571\n",
            "Iteration 1935, loss = 0.20386663\n",
            "Iteration 1936, loss = 0.20369789\n",
            "Iteration 1937, loss = 0.20352950\n",
            "Iteration 1938, loss = 0.20336146\n",
            "Iteration 1939, loss = 0.20319376\n",
            "Iteration 1940, loss = 0.20302641\n",
            "Iteration 1941, loss = 0.20285939\n",
            "Iteration 1942, loss = 0.20269272\n",
            "Iteration 1943, loss = 0.20252639\n",
            "Iteration 1944, loss = 0.20236039\n",
            "Iteration 1945, loss = 0.20219474\n",
            "Iteration 1946, loss = 0.20202942\n",
            "Iteration 1947, loss = 0.20186443\n",
            "Iteration 1948, loss = 0.20169979\n",
            "Iteration 1949, loss = 0.20153547\n",
            "Iteration 1950, loss = 0.20137149\n",
            "Iteration 1951, loss = 0.20120784\n",
            "Iteration 1952, loss = 0.20104452\n",
            "Iteration 1953, loss = 0.20088153\n",
            "Iteration 1954, loss = 0.20071887\n",
            "Iteration 1955, loss = 0.20055654\n",
            "Iteration 1956, loss = 0.20039453\n",
            "Iteration 1957, loss = 0.20023286\n",
            "Iteration 1958, loss = 0.20007150\n",
            "Iteration 1959, loss = 0.19991048\n",
            "Iteration 1960, loss = 0.19974977\n",
            "Iteration 1961, loss = 0.19958939\n",
            "Iteration 1962, loss = 0.19942933\n",
            "Iteration 1963, loss = 0.19926959\n",
            "Iteration 1964, loss = 0.19911017\n",
            "Iteration 1965, loss = 0.19895107\n",
            "Iteration 1966, loss = 0.19879229\n",
            "Iteration 1967, loss = 0.19863383\n",
            "Iteration 1968, loss = 0.19847568\n",
            "Iteration 1969, loss = 0.19831785\n",
            "Iteration 1970, loss = 0.19816033\n",
            "Iteration 1971, loss = 0.19800313\n",
            "Iteration 1972, loss = 0.19784624\n",
            "Iteration 1973, loss = 0.19768966\n",
            "Iteration 1974, loss = 0.19753340\n",
            "Iteration 1975, loss = 0.19737744\n",
            "Iteration 1976, loss = 0.19722179\n",
            "Iteration 1977, loss = 0.19706646\n",
            "Iteration 1978, loss = 0.19691143\n",
            "Iteration 1979, loss = 0.19675671\n",
            "Iteration 1980, loss = 0.19660229\n",
            "Iteration 1981, loss = 0.19644818\n",
            "Iteration 1982, loss = 0.19629438\n",
            "Iteration 1983, loss = 0.19614087\n",
            "Iteration 1984, loss = 0.19598768\n",
            "Iteration 1985, loss = 0.19583478\n",
            "Iteration 1986, loss = 0.19568219\n",
            "Iteration 1987, loss = 0.19552989\n",
            "Iteration 1988, loss = 0.19537790\n",
            "Iteration 1989, loss = 0.19522620\n",
            "Iteration 1990, loss = 0.19507480\n",
            "Iteration 1991, loss = 0.19492370\n",
            "Iteration 1992, loss = 0.19477290\n",
            "Iteration 1993, loss = 0.19462239\n",
            "Iteration 1994, loss = 0.19447218\n",
            "Iteration 1995, loss = 0.19432226\n",
            "Iteration 1996, loss = 0.19417263\n",
            "Iteration 1997, loss = 0.19402330\n",
            "Iteration 1998, loss = 0.19387426\n",
            "Iteration 1999, loss = 0.19372551\n",
            "Iteration 2000, loss = 0.19357705\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hd45BboQxTT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "adee766a-17fe-4d45-cd35-6363a82c412d"
      },
      "source": [
        "#scikit for machine learning reporting\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test,y_pred)) # Print summary report\n",
        "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))) # Print Confusion matrix \n",
        "print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       1.00      0.91      0.95        11\n",
            "           2       0.95      1.00      0.98        20\n",
            "\n",
            "   micro avg       0.98      0.98      0.98        45\n",
            "   macro avg       0.98      0.97      0.98        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            " samples avg       0.98      0.98      0.98        45\n",
            "\n",
            "[[14  0  0]\n",
            " [ 0 10  1]\n",
            " [ 0  0 20]]\n",
            "accuracy is  0.9777777777777777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJNiydUtQxWq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "792e9338-2338-478d-b245-0e1dbb3b0abe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(h.loss_curve_)\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Loss'])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3079a0f0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SGWujZrQxaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0GISD0iQxde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObSeqGvrQxg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lBWyvAmQxk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}